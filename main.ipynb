{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]2024-11-17 15:14:30,955 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:30,965 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:30,966 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:30,967 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:30,967 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:30,968 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:30,969 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:30,969 - INFO - Successfully processed GPT and TextGrad for sample 1\n",
      "2024-11-17 15:14:30,975 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:31,038 - INFO - Using default tokenizer.\n",
      "  2%|▎         | 1/40 [00:03<02:13,  3.41s/it]2024-11-17 15:14:31,100 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:31,110 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:31,111 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:31,112 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:31,112 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:31,113 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:31,114 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:31,115 - INFO - Successfully processed GPT and TextGrad for sample 3\n",
      "2024-11-17 15:14:31,120 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:31,167 - INFO - Using default tokenizer.\n",
      "  5%|▌         | 2/40 [00:03<00:56,  1.48s/it]2024-11-17 15:14:31,398 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:31,408 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:31,409 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:31,410 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:31,410 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:31,411 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:31,412 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:31,413 - INFO - Successfully processed GPT and TextGrad for sample 0\n",
      "2024-11-17 15:14:31,422 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:31,481 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:31,529 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:31,537 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:31,537 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:31,539 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:31,540 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:31,541 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:31,541 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:31,543 - INFO - Successfully processed GPT and TextGrad for sample 2\n",
      "  8%|▊         | 3/40 [00:03<00:37,  1.00s/it]2024-11-17 15:14:31,671 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:31,752 - INFO - Using default tokenizer.\n",
      " 10%|█         | 4/40 [00:04<00:24,  1.48it/s]2024-11-17 15:14:34,046 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:34,056 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:34,057 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:34,058 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:34,059 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:34,060 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:34,061 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:34,062 - INFO - Successfully processed GPT and TextGrad for sample 4\n",
      "2024-11-17 15:14:34,067 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:34,150 - INFO - Using default tokenizer.\n",
      " 12%|█▎        | 5/40 [00:06<00:45,  1.30s/it]2024-11-17 15:14:34,635 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:34,722 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:35,068 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:36,576 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:36,578 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:36,578 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:37,169 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:37,171 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:37,172 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:37,179 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:37,181 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:37,181 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:37,679 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:39,136 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:39,138 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:39,139 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:39,409 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:39,411 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:39,411 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:39,609 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:39,611 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:39,612 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:39,935 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:39,937 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:39,937 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:40,757 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:40,759 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:40,759 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:40,760 - INFO - Successfully processed GPT and TextGrad for sample 7\n",
      "2024-11-17 15:14:40,767 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:40,841 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:40,882 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:40,884 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:40,885 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:40,886 - INFO - Successfully processed GPT and TextGrad for sample 6\n",
      " 15%|█▌        | 6/40 [00:13<01:46,  3.13s/it]2024-11-17 15:14:40,939 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:41,064 - INFO - Using default tokenizer.\n",
      " 18%|█▊        | 7/40 [00:13<01:11,  2.18s/it]2024-11-17 15:14:41,263 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:41,265 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:41,266 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:41,266 - INFO - Successfully processed GPT and TextGrad for sample 5\n",
      "2024-11-17 15:14:41,275 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:41,370 - INFO - Using default tokenizer.\n",
      " 20%|██        | 8/40 [00:13<00:50,  1.58s/it]2024-11-17 15:14:42,224 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:42,226 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:42,227 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:43,200 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:14:43,201 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:14:43,562 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:43,564 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:43,564 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:43,565 - INFO - Successfully processed GPT and TextGrad for sample 8\n",
      "2024-11-17 15:14:43,574 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:43,649 - INFO - Using default tokenizer.\n",
      " 22%|██▎       | 9/40 [00:16<00:55,  1.80s/it]2024-11-17 15:14:44,404 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:44,752 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:45,985 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:45,987 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:45,988 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:46,660 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:46,662 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:46,663 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:47,278 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:47,781 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:47,783 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:47,783 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:48,496 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:48,498 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:14:48,498 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:14:48,499 - INFO - Successfully processed GPT and TextGrad for sample 9\n",
      "2024-11-17 15:14:48,505 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:14:48,567 - INFO - Using default tokenizer.\n",
      " 25%|██▌       | 10/40 [00:20<01:22,  2.75s/it]2024-11-17 15:14:49,181 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:49,183 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:49,183 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:49,450 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:14:49,451 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:14:49,666 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:49,668 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:49,668 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:49,932 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:14:49,933 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:14:52,293 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:54,156 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:54,158 - INFO - LLMCall function forward\n",
      "2024-11-17 15:14:54,159 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:14:54,419 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:14:54,420 - INFO - Retrying request to /chat/completions in 3.000000 seconds\n",
      "2024-11-17 15:14:56,432 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:56,434 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:56,434 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:14:56,730 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:14:56,731 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:14:59,877 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:14:59,879 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:14:59,879 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:15:00,145 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:00,146 - INFO - Retrying request to /chat/completions in 19.000000 seconds\n",
      "2024-11-17 15:15:01,864 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:02,138 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:02,139 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 15:15:07,969 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:07,971 - INFO - LLMCall function forward\n",
      "2024-11-17 15:15:07,971 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:15:08,250 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:08,251 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:15:08,927 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:08,929 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:15:08,930 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:15:08,930 - INFO - Successfully processed GPT and TextGrad for sample 11\n",
      "2024-11-17 15:15:08,937 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:15:09,008 - INFO - Using default tokenizer.\n",
      " 28%|██▊       | 11/40 [00:41<03:57,  8.17s/it]2024-11-17 15:15:12,502 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:12,759 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:12,761 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:15:15,002 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:15,004 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:15:15,005 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:15:15,006 - INFO - Successfully processed GPT and TextGrad for sample 12\n",
      "2024-11-17 15:15:15,013 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:15:15,106 - INFO - Using default tokenizer.\n",
      " 30%|███       | 12/40 [00:47<03:31,  7.54s/it]2024-11-17 15:15:18,949 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:19,502 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:19,504 - INFO - LLMCall function forward\n",
      "2024-11-17 15:15:19,504 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:15:19,792 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:19,792 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 15:15:20,955 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:20,957 - INFO - LLMCall function forward\n",
      "2024-11-17 15:15:20,957 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:15:21,050 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:21,052 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:15:21,053 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:15:21,053 - INFO - Successfully processed GPT and TextGrad for sample 13\n",
      "2024-11-17 15:15:21,060 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:15:21,141 - INFO - Using default tokenizer.\n",
      " 32%|███▎      | 13/40 [00:53<03:11,  7.09s/it]2024-11-17 15:15:21,217 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:21,218 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 15:15:24,509 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:24,510 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:15:24,619 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:26,499 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:26,500 - INFO - LLMCall function forward\n",
      "2024-11-17 15:15:26,501 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:15:26,754 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:26,754 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 15:15:27,066 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:27,067 - INFO - Retrying request to /chat/completions in 6.000000 seconds\n",
      "2024-11-17 15:15:35,183 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:35,186 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:15:35,186 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:15:35,486 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:35,487 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 15:15:36,234 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:36,236 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:15:36,237 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:15:36,530 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:36,531 - INFO - Retrying request to /chat/completions in 33.000000 seconds\n",
      "2024-11-17 15:15:37,684 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:37,686 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:15:37,687 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:15:37,971 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:37,972 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:15:39,743 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:39,744 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 15:15:41,099 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:41,101 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:15:41,102 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:15:41,365 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:41,366 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 15:15:55,940 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:55,942 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:15:55,942 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:15:55,943 - INFO - Successfully processed GPT and TextGrad for sample 10\n",
      "2024-11-17 15:15:55,950 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:15:56,020 - INFO - Using default tokenizer.\n",
      " 35%|███▌      | 14/40 [01:28<06:42, 15.48s/it]2024-11-17 15:15:56,264 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:15:56,265 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:15:56,266 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:15:56,267 - INFO - Successfully processed GPT and TextGrad for sample 14\n",
      "2024-11-17 15:15:56,273 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:15:56,362 - INFO - Using default tokenizer.\n",
      " 38%|███▊      | 15/40 [01:28<04:32, 10.92s/it]2024-11-17 15:15:58,550 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:15:58,551 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 15:15:59,838 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:01,445 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:01,447 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:16:01,448 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:16:01,448 - INFO - Successfully processed GPT and TextGrad for sample 16\n",
      "2024-11-17 15:16:01,457 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:16:01,523 - INFO - Using default tokenizer.\n",
      " 40%|████      | 16/40 [01:33<03:40,  9.19s/it]2024-11-17 15:16:01,987 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:02,008 - INFO - LLMCall function forward\n",
      "2024-11-17 15:16:02,009 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:16:04,372 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:04,374 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:16:04,375 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:16:04,671 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:04,672 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 15:16:05,018 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:06,810 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:06,812 - INFO - LLMCall function forward\n",
      "2024-11-17 15:16:06,813 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:16:07,103 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:07,104 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:16:09,812 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:09,813 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 15:16:10,292 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:10,294 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:16:10,294 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:16:10,295 - INFO - Successfully processed GPT and TextGrad for sample 17\n",
      "2024-11-17 15:16:10,302 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:16:10,401 - INFO - Using default tokenizer.\n",
      " 42%|████▎     | 17/40 [01:42<03:29,  9.11s/it]2024-11-17 15:16:13,442 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:13,705 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:13,706 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:16:13,867 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:15,874 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:15,877 - INFO - LLMCall function forward\n",
      "2024-11-17 15:16:15,878 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:16:16,162 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:16,163 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:16:19,565 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:19,567 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:16:19,568 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:16:19,823 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:19,824 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-11-17 15:16:20,591 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:20,593 - INFO - LLMCall function forward\n",
      "2024-11-17 15:16:20,594 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:16:20,854 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:20,856 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 15:16:22,077 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:23,330 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:23,331 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:16:27,086 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:27,088 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:16:27,089 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:16:27,162 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:27,164 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:16:27,165 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:16:27,365 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:27,366 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 15:16:27,463 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:27,464 - INFO - Retrying request to /chat/completions in 33.000000 seconds\n",
      "2024-11-17 15:16:40,870 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:40,872 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:16:40,873 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:16:40,874 - INFO - Successfully processed GPT and TextGrad for sample 15\n",
      "2024-11-17 15:16:40,881 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:16:40,976 - INFO - Using default tokenizer.\n",
      " 45%|████▌     | 18/40 [02:13<05:41, 15.54s/it]2024-11-17 15:16:41,686 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:41,688 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:16:41,688 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:16:41,689 - INFO - Successfully processed GPT and TextGrad for sample 19\n",
      "2024-11-17 15:16:41,696 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:16:41,767 - INFO - Using default tokenizer.\n",
      " 48%|████▊     | 19/40 [02:14<03:53, 11.12s/it]2024-11-17 15:16:42,777 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:42,779 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:16:42,779 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:16:42,780 - INFO - Successfully processed GPT and TextGrad for sample 18\n",
      "2024-11-17 15:16:42,787 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:16:42,863 - INFO - Using default tokenizer.\n",
      " 50%|█████     | 20/40 [02:15<02:42,  8.11s/it]2024-11-17 15:16:44,382 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:44,882 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:45,052 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:45,053 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 15:16:45,144 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:45,145 - INFO - Retrying request to /chat/completions in 1.000000 seconds\n",
      "2024-11-17 15:16:46,954 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:46,956 - INFO - LLMCall function forward\n",
      "2024-11-17 15:16:46,956 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:16:47,261 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:47,262 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:16:47,514 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:16:47,516 - INFO - LLMCall function forward\n",
      "2024-11-17 15:16:47,517 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:16:47,772 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:47,773 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 15:16:58,527 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:16:58,527 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:17:01,743 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:01,921 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:01,922 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:17:01,923 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:17:01,924 - INFO - Successfully processed GPT and TextGrad for sample 20\n",
      "2024-11-17 15:17:01,932 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:17:02,042 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:02,046 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:17:02,046 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      " 52%|█████▎    | 21/40 [02:34<03:37, 11.43s/it]2024-11-17 15:17:02,858 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:02,860 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:17:02,861 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:17:03,110 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:03,110 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 15:17:03,785 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:05,056 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:05,057 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 15:17:05,505 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:05,762 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:05,763 - INFO - Retrying request to /chat/completions in 1.000000 seconds\n",
      "2024-11-17 15:17:08,419 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:08,421 - INFO - LLMCall function forward\n",
      "2024-11-17 15:17:08,421 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:17:08,703 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:08,704 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:17:09,282 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:09,284 - INFO - LLMCall function forward\n",
      "2024-11-17 15:17:09,285 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:17:09,549 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:09,550 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 15:17:18,971 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:18,973 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:17:18,974 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:17:19,314 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:19,316 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:17:19,316 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:17:19,317 - INFO - Successfully processed GPT and TextGrad for sample 22\n",
      "2024-11-17 15:17:19,324 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:17:19,391 - INFO - Using default tokenizer.\n",
      " 55%|█████▌    | 22/40 [02:51<03:57, 13.21s/it]2024-11-17 15:17:19,851 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:19,853 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:17:19,854 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:17:19,855 - INFO - Successfully processed GPT and TextGrad for sample 21\n",
      "2024-11-17 15:17:19,863 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:17:19,923 - INFO - Using default tokenizer.\n",
      " 57%|█████▊    | 23/40 [02:52<02:39,  9.40s/it]2024-11-17 15:17:22,158 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:22,158 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 15:17:22,955 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:25,092 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:25,094 - INFO - LLMCall function forward\n",
      "2024-11-17 15:17:25,095 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:17:26,776 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:26,778 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:17:26,779 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:17:27,077 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:27,078 - INFO - Retrying request to /chat/completions in 2.000000 seconds\n",
      "2024-11-17 15:17:27,121 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:27,123 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:17:27,124 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:17:27,377 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:27,378 - INFO - Retrying request to /chat/completions in 35.000000 seconds\n",
      "2024-11-17 15:17:27,565 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:27,567 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:17:27,568 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:17:28,934 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:28,936 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:17:28,937 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:17:28,937 - INFO - Successfully processed GPT and TextGrad for sample 25\n",
      "2024-11-17 15:17:28,944 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:17:29,015 - INFO - Using default tokenizer.\n",
      " 60%|██████    | 24/40 [03:01<02:28,  9.31s/it]2024-11-17 15:17:30,532 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:30,534 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:17:30,535 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:17:30,535 - INFO - Successfully processed GPT and TextGrad for sample 23\n",
      "2024-11-17 15:17:30,544 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:17:30,661 - INFO - Using default tokenizer.\n",
      " 62%|██████▎   | 25/40 [03:03<01:45,  7.01s/it]2024-11-17 15:17:31,969 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:32,225 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:32,227 - INFO - Retrying request to /chat/completions in 2.000000 seconds\n",
      "2024-11-17 15:17:32,822 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:32,822 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:17:35,655 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:35,925 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:35,926 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 15:17:36,138 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:36,140 - INFO - LLMCall function forward\n",
      "2024-11-17 15:17:36,141 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:17:36,402 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:36,403 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:17:41,619 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:41,621 - INFO - LLMCall function forward\n",
      "2024-11-17 15:17:41,622 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:17:41,881 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:41,882 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 15:17:45,365 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:45,620 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:45,621 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:17:52,461 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:52,463 - INFO - LLMCall function forward\n",
      "2024-11-17 15:17:52,464 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:17:52,700 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:52,701 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:17:52,734 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:52,735 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 15:17:58,926 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:17:58,928 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:17:58,928 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:17:59,193 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:17:59,194 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:18:00,286 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:00,288 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:18:00,289 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:18:02,101 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:02,102 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:18:02,103 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:18:02,104 - INFO - Successfully processed GPT and TextGrad for sample 27\n",
      "2024-11-17 15:18:02,109 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:18:02,169 - INFO - Using default tokenizer.\n",
      " 65%|██████▌   | 26/40 [03:34<03:21, 14.36s/it]2024-11-17 15:18:02,665 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:02,666 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:18:05,661 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:07,517 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:07,519 - INFO - LLMCall function forward\n",
      "2024-11-17 15:18:07,520 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:18:10,149 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:10,151 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:18:10,152 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:18:10,231 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:10,233 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:18:10,233 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:18:10,412 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:10,413 - INFO - Retrying request to /chat/completions in 2.000000 seconds\n",
      "2024-11-17 15:18:10,513 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:10,514 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:18:12,970 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:14,221 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:14,222 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 15:18:14,663 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:14,665 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:18:14,665 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:18:14,666 - INFO - Successfully processed GPT and TextGrad for sample 29\n",
      "2024-11-17 15:18:14,673 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:18:14,747 - INFO - Using default tokenizer.\n",
      " 68%|██████▊   | 27/40 [03:47<02:59, 13.83s/it]2024-11-17 15:18:15,491 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:15,492 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:18:18,385 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:18,640 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:18,641 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:18:25,992 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:25,994 - INFO - LLMCall function forward\n",
      "2024-11-17 15:18:25,995 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:18:26,252 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:26,253 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 15:18:29,411 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:29,413 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:18:29,414 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:18:29,414 - INFO - Successfully processed GPT and TextGrad for sample 28\n",
      "2024-11-17 15:18:29,423 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:18:29,503 - INFO - Using default tokenizer.\n",
      " 70%|███████   | 28/40 [04:01<02:49, 14.11s/it]2024-11-17 15:18:30,634 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:30,637 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:18:30,637 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:18:30,638 - INFO - Successfully processed GPT and TextGrad for sample 24\n",
      "2024-11-17 15:18:30,645 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:18:30,919 - INFO - Using default tokenizer.\n",
      " 72%|███████▎  | 29/40 [04:03<01:53, 10.30s/it]2024-11-17 15:18:32,684 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:32,858 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:32,925 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:32,926 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 15:18:34,122 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:34,123 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:18:35,032 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:35,034 - INFO - LLMCall function forward\n",
      "2024-11-17 15:18:35,034 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:18:37,514 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:37,516 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:18:37,516 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:18:37,812 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:37,813 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:18:46,451 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:46,453 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:18:46,454 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:18:46,751 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:46,752 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 15:18:47,003 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:47,004 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:18:47,005 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:18:47,006 - INFO - Successfully processed GPT and TextGrad for sample 26\n",
      "2024-11-17 15:18:47,012 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:18:47,081 - INFO - Using default tokenizer.\n",
      " 75%|███████▌  | 30/40 [04:19<02:00, 12.05s/it]2024-11-17 15:18:48,173 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:48,249 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:48,252 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:18:48,253 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:18:48,253 - INFO - Successfully processed GPT and TextGrad for sample 31\n",
      "2024-11-17 15:18:48,260 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:18:48,355 - INFO - Using default tokenizer.\n",
      " 78%|███████▊  | 31/40 [04:20<01:19,  8.82s/it]2024-11-17 15:18:48,443 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:48,444 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 15:18:50,293 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:50,537 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:50,538 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 15:18:52,148 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:52,150 - INFO - LLMCall function forward\n",
      "2024-11-17 15:18:52,151 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:18:54,267 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:54,269 - INFO - LLMCall function forward\n",
      "2024-11-17 15:18:54,270 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:18:54,940 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:54,942 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:18:54,942 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:18:55,221 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:55,221 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:18:55,981 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:18:55,983 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:18:55,984 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:18:56,245 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:18:56,246 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:19:05,067 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:05,068 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:19:05,499 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:05,500 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 15:19:06,132 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:06,412 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:06,413 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 15:19:13,214 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:13,216 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:19:13,217 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:19:13,217 - INFO - Successfully processed GPT and TextGrad for sample 32\n",
      "2024-11-17 15:19:13,225 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:19:13,302 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:19:13,302 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:13,309 - INFO - LLMCall function forward\n",
      "2024-11-17 15:19:13,309 - INFO - _backward_through_llm prompt\n",
      " 80%|████████  | 32/40 [04:45<01:49, 13.65s/it]2024-11-17 15:19:13,568 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:13,569 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:19:17,231 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:19,219 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:19,221 - INFO - LLMCall function forward\n",
      "2024-11-17 15:19:19,222 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:19:21,153 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:21,155 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:19:21,156 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:19:21,427 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:21,428 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 15:19:21,926 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:21,928 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:19:21,928 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:19:21,929 - INFO - Successfully processed GPT and TextGrad for sample 33\n",
      "2024-11-17 15:19:21,936 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:19:22,054 - INFO - Using default tokenizer.\n",
      " 82%|████████▎ | 33/40 [04:54<01:25, 12.20s/it]2024-11-17 15:19:22,425 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:23,721 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:23,722 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:19:25,521 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:27,535 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:27,537 - INFO - LLMCall function forward\n",
      "2024-11-17 15:19:27,537 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:19:27,795 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:27,796 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 15:19:30,821 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:30,824 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:19:30,825 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:19:30,825 - INFO - Successfully processed GPT and TextGrad for sample 35\n",
      "2024-11-17 15:19:30,832 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:19:30,919 - INFO - Using default tokenizer.\n",
      " 85%|████████▌ | 34/40 [05:03<01:07, 11.20s/it]2024-11-17 15:19:32,048 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:32,050 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:19:32,051 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:19:32,349 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:32,350 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-11-17 15:19:33,828 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:35,019 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:35,020 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 15:19:36,245 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:36,247 - INFO - LLMCall function forward\n",
      "2024-11-17 15:19:36,248 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:19:36,539 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:36,540 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 15:19:41,921 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:41,923 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:19:41,923 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:19:42,220 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:42,221 - INFO - Retrying request to /chat/completions in 19.000000 seconds\n",
      "2024-11-17 15:19:45,598 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:45,600 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:19:45,600 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:19:45,898 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:45,899 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 15:19:52,437 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:52,439 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:19:52,439 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:19:52,440 - INFO - Successfully processed GPT and TextGrad for sample 30\n",
      "2024-11-17 15:19:52,450 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:19:52,570 - INFO - Using default tokenizer.\n",
      " 88%|████████▊ | 35/40 [05:24<01:11, 14.32s/it]2024-11-17 15:19:52,648 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:52,649 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 15:19:56,373 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:19:56,624 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:19:56,625 - INFO - Retrying request to /chat/completions in 6.000000 seconds\n",
      "2024-11-17 15:20:02,858 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:02,860 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:20:02,861 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:20:02,861 - INFO - Successfully processed GPT and TextGrad for sample 36\n",
      "2024-11-17 15:20:02,872 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:02,965 - INFO - Using default tokenizer.\n",
      " 90%|█████████ | 36/40 [05:35<00:52, 13.15s/it]2024-11-17 15:20:04,573 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:04,575 - INFO - LLMCall function forward\n",
      "2024-11-17 15:20:04,576 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:20:04,834 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:20:04,835 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 15:20:04,875 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:04,877 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:20:04,878 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:20:04,878 - INFO - Successfully processed GPT and TextGrad for sample 37\n",
      "2024-11-17 15:20:04,883 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:04,958 - INFO - Using default tokenizer.\n",
      " 92%|█████████▎| 37/40 [05:37<00:29,  9.80s/it]2024-11-17 15:20:06,665 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:08,601 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:08,603 - INFO - LLMCall function forward\n",
      "2024-11-17 15:20:08,603 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 15:20:10,749 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:10,751 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:20:10,751 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:20:10,927 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:20:11,007 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:20:11,008 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 15:20:12,187 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:20:12,188 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 15:20:16,946 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:16,948 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:20:16,949 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:20:16,949 - INFO - Successfully processed GPT and TextGrad for sample 39\n",
      "2024-11-17 15:20:16,956 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:17,037 - INFO - Using default tokenizer.\n",
      " 95%|█████████▌| 38/40 [05:49<00:20, 10.48s/it]2024-11-17 15:20:24,326 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:24,328 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 15:20:24,328 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 15:20:24,601 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:20:24,602 - INFO - Retrying request to /chat/completions in 21.000000 seconds\n",
      "2024-11-17 15:20:26,468 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:20:26,469 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 15:20:46,273 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:46,275 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:20:46,275 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:20:46,276 - INFO - Successfully processed GPT and TextGrad for sample 34\n",
      "2024-11-17 15:20:46,282 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:46,364 - INFO - Using default tokenizer.\n",
      " 98%|█████████▊| 39/40 [06:18<00:16, 16.14s/it]2024-11-17 15:20:48,965 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 15:20:48,967 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 15:20:48,968 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 15:20:48,968 - INFO - Successfully processed GPT and TextGrad for sample 38\n",
      "2024-11-17 15:20:48,976 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:49,099 - INFO - Using default tokenizer.\n",
      "100%|██████████| 40/40 [06:21<00:00,  9.54s/it]\n",
      "2024-11-17 15:20:49,238 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:49,339 - INFO - Successfully added DSPy response (3205 chars) for sample 1\n",
      "2024-11-17 15:20:49,349 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:49,413 - INFO - Successfully added DSPy response (3075 chars) for sample 3\n",
      "2024-11-17 15:20:49,422 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:49,538 - INFO - Successfully added DSPy response (2381 chars) for sample 0\n",
      "2024-11-17 15:20:49,548 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:49,648 - INFO - Successfully added DSPy response (2949 chars) for sample 2\n",
      "2024-11-17 15:20:49,658 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:49,755 - INFO - Successfully added DSPy response (2900 chars) for sample 4\n",
      "\u001b[92m15:20:49 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:20:49,758 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:20:52,397 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:52 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:20:52,410 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:20:52,417 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:52,515 - INFO - Successfully added DSPy response (2231 chars) for sample 7\n",
      "\u001b[92m15:20:52 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:20:52,518 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:20:55,804 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:55 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:20:55,807 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:20:55,816 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:55,919 - INFO - Successfully added DSPy response (2932 chars) for sample 6\n",
      "\u001b[92m15:20:55 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:20:55,922 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:20:59,141 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:20:59,143 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:20:59,152 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:20:59,232 - INFO - Successfully added DSPy response (2900 chars) for sample 5\n",
      "\u001b[92m15:20:59 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:20:59,234 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:02,120 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:02 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:02,122 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:02,130 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:21:02,224 - INFO - Successfully added DSPy response (2524 chars) for sample 8\n",
      "\u001b[92m15:21:02 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:02,226 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:05,365 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:05 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:05,367 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:05,376 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:21:05,449 - INFO - Successfully added DSPy response (2963 chars) for sample 9\n",
      "\u001b[92m15:21:05 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:05,452 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:05,704 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:21:05,704 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 15:21:16,428 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:16 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:16,431 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:16,439 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:21:16,537 - INFO - Successfully added DSPy response (2571 chars) for sample 11\n",
      "\u001b[92m15:21:16 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:16,539 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:16,810 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:21:16,811 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:21:28,763 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:28,766 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:28,775 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:21:28,886 - INFO - Successfully added DSPy response (2706 chars) for sample 12\n",
      "\u001b[92m15:21:28 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:28,889 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:29,148 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:21:29,149 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:21:41,112 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:41 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:41,114 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:41,123 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:21:41,226 - INFO - Successfully added DSPy response (2581 chars) for sample 13\n",
      "\u001b[92m15:21:41 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:41,228 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:41,484 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:21:41,485 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:21:54,175 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:54,177 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:21:54,186 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:21:54,276 - INFO - Successfully added DSPy response (2445 chars) for sample 10\n",
      "\u001b[92m15:21:54 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:54,278 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:21:54,530 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:21:54,531 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:22:06,837 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:06 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:06,839 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:06,848 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:22:06,956 - INFO - Successfully added DSPy response (2988 chars) for sample 14\n",
      "\u001b[92m15:22:06 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:06,958 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:07,215 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:22:07,216 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:22:21,322 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:21 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:21,324 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:21,333 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:22:21,407 - INFO - Successfully added DSPy response (2677 chars) for sample 16\n",
      "\u001b[92m15:22:21 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:21,409 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:21,663 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:22:21,664 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:22:34,553 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:34 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:34,556 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:34,564 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:22:34,685 - INFO - Successfully added DSPy response (2612 chars) for sample 17\n",
      "\u001b[92m15:22:34 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:34,688 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:34,953 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:22:34,954 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:22:46,820 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:46 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:46,823 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:46,832 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:22:46,926 - INFO - Successfully added DSPy response (2683 chars) for sample 15\n",
      "\u001b[92m15:22:46 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:46,929 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:47,199 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:22:47,200 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:22:59,487 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:22:59 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:59,490 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:22:59,500 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:22:59,600 - INFO - Successfully added DSPy response (2839 chars) for sample 19\n",
      "\u001b[92m15:22:59 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:59,603 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:22:59,873 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:22:59,874 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:23:13,477 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:13 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:13,479 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:13,488 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:23:13,589 - INFO - Successfully added DSPy response (3522 chars) for sample 18\n",
      "\u001b[92m15:23:13 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:13,592 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:13,843 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:23:13,844 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:23:28,563 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:28,566 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:28,575 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:23:28,680 - INFO - Successfully added DSPy response (3452 chars) for sample 20\n",
      "\u001b[92m15:23:28 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:28,682 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:28,931 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:23:28,931 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:23:42,550 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:42 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:42,553 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:42,561 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:23:42,644 - INFO - Successfully added DSPy response (2385 chars) for sample 22\n",
      "\u001b[92m15:23:42 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:42,646 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:42,909 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:23:42,910 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:23:55,051 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:23:55 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:55,054 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:23:55,062 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:23:55,125 - INFO - Successfully added DSPy response (2500 chars) for sample 21\n",
      "\u001b[92m15:23:55 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:55,127 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:23:55,391 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:23:55,392 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:24:07,243 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:07 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:07,246 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:07,256 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:24:07,337 - INFO - Successfully added DSPy response (2601 chars) for sample 25\n",
      "\u001b[92m15:24:07 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:07,339 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:07,605 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:24:07,606 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:24:21,366 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:21 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:21,369 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:21,377 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:24:21,465 - INFO - Successfully added DSPy response (2163 chars) for sample 23\n",
      "\u001b[92m15:24:21 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:21,467 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:21,741 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:24:21,741 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:24:34,166 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:34 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:34,169 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:34,178 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:24:34,274 - INFO - Successfully added DSPy response (3044 chars) for sample 27\n",
      "\u001b[92m15:24:34 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:34,276 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:34,546 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:24:34,547 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:24:48,554 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:24:48 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:48,557 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:24:48,567 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:24:48,658 - INFO - Successfully added DSPy response (2691 chars) for sample 29\n",
      "\u001b[92m15:24:48 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:48,660 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:24:48,912 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:24:48,913 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:25:00,602 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:00 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:00,604 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:00,612 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:25:00,692 - INFO - Successfully added DSPy response (2374 chars) for sample 28\n",
      "\u001b[92m15:25:00 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:00,694 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:00,956 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:25:00,957 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:25:12,748 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:12,750 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:12,759 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:25:12,862 - INFO - Successfully added DSPy response (2473 chars) for sample 24\n",
      "\u001b[92m15:25:12 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:12,865 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:13,128 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:25:13,129 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:25:25,063 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:25 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:25,066 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:25,073 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:25:25,154 - INFO - Successfully added DSPy response (2431 chars) for sample 26\n",
      "\u001b[92m15:25:25 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:25,156 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:25,422 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:25:25,423 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:25:38,466 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:38 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:38,469 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:38,481 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:25:38,574 - INFO - Successfully added DSPy response (2994 chars) for sample 31\n",
      "\u001b[92m15:25:38 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:38,577 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:38,840 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:25:38,841 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:25:51,190 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:25:51 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:51,193 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:25:51,202 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:25:51,299 - INFO - Successfully added DSPy response (3285 chars) for sample 32\n",
      "\u001b[92m15:25:51 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:51,302 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:25:51,562 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:25:51,563 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:26:05,433 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:05 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:05,435 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:05,444 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:26:05,548 - INFO - Successfully added DSPy response (2500 chars) for sample 33\n",
      "\u001b[92m15:26:05 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:05,551 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:05,824 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:26:05,825 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:26:18,044 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:18 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:18,046 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:18,055 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:26:18,159 - INFO - Successfully added DSPy response (2997 chars) for sample 35\n",
      "\u001b[92m15:26:18 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:18,162 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:18,427 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:26:18,428 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:26:31,215 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:31 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:31,217 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:31,226 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:26:31,320 - INFO - Successfully added DSPy response (2555 chars) for sample 30\n",
      "\u001b[92m15:26:31 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:31,323 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:31,579 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:26:31,579 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:26:44,848 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:44 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:44,851 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:44,860 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:26:44,959 - INFO - Successfully added DSPy response (3078 chars) for sample 36\n",
      "\u001b[92m15:26:44 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:44,961 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:45,215 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:26:45,216 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 15:26:57,026 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:26:57 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:57,029 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:26:57,037 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:26:57,126 - INFO - Successfully added DSPy response (2322 chars) for sample 37\n",
      "\u001b[92m15:26:57 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:57,128 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:26:57,394 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:26:57,395 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 15:27:11,034 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:11 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:27:11,037 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:27:11,047 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:27:11,151 - INFO - Successfully added DSPy response (3282 chars) for sample 39\n",
      "\u001b[92m15:27:11 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:27:11,153 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:27:11,431 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:27:11,432 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 15:27:25,664 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:25 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:27:25,667 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:27:25,676 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:27:25,785 - INFO - Successfully added DSPy response (2951 chars) for sample 34\n",
      "\u001b[92m15:27:25 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:27:25,787 - INFO - \n",
      "LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq\n",
      "2024-11-17 15:27:26,039 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 15:27:26,040 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 15:27:41,382 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:27:41 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:27:41,386 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 15:27:41,399 - INFO - Using default tokenizer.\n",
      "2024-11-17 15:27:41,519 - INFO - Successfully added DSPy response (2961 chars) for sample 38\n",
      "2024-11-17 15:27:41,527 - INFO - Results saved to 'final_results.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import textgrad as tg\n",
    "from textgrad.engine.local_model_openai_api import ChatExternalClient\n",
    "from groq import Groq\n",
    "import dspy\n",
    "from dspy.primitives.program import Module\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import yake\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import itertools\n",
    "from pydantic import BaseModel, Field\n",
    "import threading\n",
    "from dspy.predict import ChainOfThought\n",
    "\n",
    "dspy_config_lock = threading.Lock()\n",
    "dspy_configured = threading.Event()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "key_lock = threading.Lock()\n",
    "\n",
    "# Rate limiting constants\n",
    "RATE_LIMIT = 30  # requests per minute per API key\n",
    "DELAY_BETWEEN_REQUESTS = 60 / RATE_LIMIT  # seconds between requests\n",
    "BATCH_SIZE = 25  # Slightly less than rate limit to account for overhead\n",
    "API_KEYS = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"\n",
    "]\n",
    "api_key_cycle = itertools.cycle(API_KEYS)\n",
    "key_lock = Lock()\n",
    "\n",
    "def ensure_nltk_data():\n",
    "    \"\"\"Ensure all required NLTK data is downloaded\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "\n",
    "def initialize_dspy(api_key):\n",
    "    \"\"\"Initialize DSPy configuration once\"\"\"\n",
    "    global dspy_configured\n",
    "    with dspy_config_lock:\n",
    "        if not dspy_configured.is_set():\n",
    "            try:\n",
    "                dspy.settings.configure(lm=dspy.OpenAI(\n",
    "                    model=\"llama-3.1-70b-versatile\",\n",
    "                    api_key=api_key,\n",
    "                    api_base=\"https://api.groq.com/openai/v1\"\n",
    "                ))\n",
    "                dspy_configured.set()\n",
    "                logging.info(\"DSPy configured successfully\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to configure DSPy: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "class Input(BaseModel):\n",
    "    context: str = Field(description=\"The context for the question\")\n",
    "    query: str = Field(description=\"The question to be answered\")\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str = Field(description=\"The answer for the question\")\n",
    "    confidence: float = Field(ge=0, le=1, description=\"The confidence score for the answer\")\n",
    "\n",
    "class MedicalResponse(dspy.Signature):\n",
    "    \"\"\"Generate accurate medical responses with reasoning.\"\"\"\n",
    "    input: Input = dspy.InputField()\n",
    "    output: Output = dspy.OutputField()\n",
    "\n",
    "class GroqPredictor(dspy.TypedPredictor):\n",
    "    \"\"\"Custom predictor for Groq API\"\"\"\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Groq(api_key=api_key)\n",
    "        \n",
    "    def forward(self, prompt):\n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Groq API call failed: {e}\")\n",
    "            return None\n",
    "\n",
    "def get_gpt_response(client, prompt, retry_count=3):\n",
    "    \"\"\"Get GPT response with rate limiting and retries\"\"\"\n",
    "    system_prompt = \"\"\"You are a medical expert assistant. Provide accurate, clear, and well-structured medical advice.\n",
    "Focus on: accuracy, clear explanation, practical advice, and professional yet accessible language.\n",
    "Please limit your response to a maximum of 1500 characters.\"\"\"\n",
    "    \n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"GPT attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(5)\n",
    "    return None\n",
    "\n",
    "def get_textgrad_response(client, prompt, reference, retry_count=3):\n",
    "    \"\"\"Get TextGrad optimized response\"\"\"\n",
    "    try:\n",
    "        engine = ChatExternalClient(client=client, model_string='llama-3.1-70b-versatile')\n",
    "        tg.set_backward_engine(engine, override=True)\n",
    "\n",
    "        loss_system_prompt = tg.Variable(\n",
    "            \"\"\"Evaluate the medical response based on the following criteria:\n",
    "            - Accuracy: Ensure all information is factually correct and evidence-based.\n",
    "            - Clarity: The response should be clear, concise, and easy to understand.\n",
    "            - Completeness: Address all aspects of the medical query comprehensively.\n",
    "            - Practicality: Provide actionable and practical advice that can be implemented.\n",
    "            - Professionalism: Maintain a professional tone and uphold medical ethical standards.\n",
    "            - Relevance: Ensure all information provided is directly related to the query.\n",
    "            - Consistency: Maintain consistency in terminology and presentation throughout the response.\n",
    "            \"\"\",\n",
    "            requires_grad=False,\n",
    "            role_description=\"medical evaluation system\"\n",
    "        )\n",
    "\n",
    "        solution = tg.Variable(reference, requires_grad=True, role_description=\"medical response\")\n",
    "        loss_fn = tg.TextLoss(loss_system_prompt)\n",
    "        optimizer = tg.TGD([solution])\n",
    "        loss = loss_fn(solution)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return solution.value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"TextGrad optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_dspy_response(api_key, input_text):\n",
    "    \"\"\"Get response using DSPy framework\"\"\"\n",
    "    try:\n",
    "        # Ensure DSPy is initialized\n",
    "        initialize_dspy(api_key)\n",
    "        \n",
    "        # Define the signature\n",
    "        class MedicalResponse(dspy.Signature):\n",
    "            input = dspy.InputField(desc=\"medical query or symptom description\")\n",
    "            response = dspy.OutputField(desc=\"detailed medical advice\")\n",
    "\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "        # Create program instance with proper error handling\n",
    "        try:\n",
    "            program = ChainOfThought(MedicalResponse)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to create ChainOfThought program: {str(e)}\")\n",
    "            return {'response': None}\n",
    "\n",
    "        # Generate response with timeout\n",
    "        try:\n",
    "            pred = program(input=str(input_text))\n",
    "            \n",
    "            if hasattr(pred, 'response') and pred.response:\n",
    "                response_text = str(pred.response).strip()\n",
    "                if response_text:\n",
    "                    logging.info(\"Successfully generated DSPy response\")\n",
    "                    return {'response': response_text}\n",
    "            \n",
    "            logging.warning(\"DSPy generated empty response\")\n",
    "            return {'response': None}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate DSPy response: {str(e)}\")\n",
    "            return {'response': None}\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"DSPy processing failed: {str(e)}\")\n",
    "        logging.error(f\"Input text was: {str(input_text)[:100]}...\")\n",
    "        return {'response': None}\n",
    "\n",
    "\n",
    "def calculate_metrics(reference, candidate):\n",
    "    \"\"\"Calculate various similarity metrics between reference and candidate texts\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if not reference or not candidate:\n",
    "        return {\n",
    "            'content_similarity': 0,\n",
    "            'word_overlap': 0,\n",
    "            'bleu_score': 0,\n",
    "            'rouge1': 0,\n",
    "            'rouge2': 0,\n",
    "            'rougeL': 0,\n",
    "            'keyword_overlap': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Content Similarity\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([reference, candidate])\n",
    "        metrics['content_similarity'] = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        \n",
    "        # Word Overlap\n",
    "        ref_words = set(reference.lower().split())\n",
    "        cand_words = set(candidate.lower().split())\n",
    "        metrics['word_overlap'] = len(ref_words.intersection(cand_words)) / len(ref_words) if ref_words else 0\n",
    "        \n",
    "        # BLEU Score\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        metrics['bleu_score'] = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n",
    "        \n",
    "        # ROUGE Scores\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = scorer.score(reference, candidate)\n",
    "        metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
    "        metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
    "        metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
    "        \n",
    "        # Keyword Matching\n",
    "        kw_extractor = yake.KeywordExtractor()\n",
    "        ref_keywords = {kw[0] for kw in kw_extractor.extract_keywords(reference)}\n",
    "        cand_keywords = {kw[0] for kw in kw_extractor.extract_keywords(candidate)}\n",
    "        metrics['keyword_overlap'] = len(ref_keywords.intersection(cand_keywords)) / len(ref_keywords) if ref_keywords else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Metrics calculation failed: {e}\")\n",
    "        metrics = {k: 0 for k in ['content_similarity', 'word_overlap', 'bleu_score', 'rouge1', 'rouge2', 'rougeL', 'keyword_overlap']}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def limit_response(response, limit=1500):\n",
    "    \"\"\"Truncate the response to the specified character limit\"\"\"\n",
    "    return response[:limit] if response and len(response) > limit else response\n",
    "\n",
    "\n",
    "def process_sample(row, start_time):\n",
    "    \"\"\"Process a single sample without threading for DSPy\"\"\"\n",
    "    try:\n",
    "        with key_lock:\n",
    "            api_key = next(api_key_cycle)\n",
    "        \n",
    "        # Initialize clients\n",
    "        groq_client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=api_key)\n",
    "        \n",
    "        input_text = str(row['input']) if pd.notna(row['input']) else \"\"\n",
    "        output_text = str(row['output']) if pd.notna(row['output']) else \"\"\n",
    "        \n",
    "        # Get GPT and TextGrad responses (these are thread-safe)\n",
    "        gpt_response = None\n",
    "        textgrad_response = None\n",
    "        \n",
    "        try:\n",
    "            gpt_response = get_gpt_response(groq_client, input_text)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"GPT response failed: {str(e)}\")\n",
    "            \n",
    "        try:\n",
    "            textgrad_response = get_textgrad_response(groq_client, input_text, output_text)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"TextGrad response failed: {str(e)}\")\n",
    "        \n",
    "        result = {\n",
    "            'index': int(row.name) if isinstance(row.name, (int, float)) else 0,\n",
    "            'input': input_text,\n",
    "            'reference_output': output_text,\n",
    "            'responses': {\n",
    "                'gpt': str(gpt_response) if gpt_response is not None else None,\n",
    "                'textgrad': str(textgrad_response) if textgrad_response is not None else None,\n",
    "                'dspy': None  # We'll fill this in later\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Successfully processed GPT and TextGrad for sample {row.name}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing sample {row.name}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing sample {row.name}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def estimate_completion_time(total_samples):\n",
    "    \"\"\"Calculate estimated completion time based on rate limits\"\"\"\n",
    "    # Each sample needs 3 API calls (GPT, TextGrad, and DSPy)\n",
    "    total_requests = total_samples * 3\n",
    "    \n",
    "    # Calculate total minutes needed\n",
    "    total_minutes = total_requests / RATE_LIMIT\n",
    "    \n",
    "    # Add 20% buffer for overhead and potential retries\n",
    "    total_minutes *= 1.2\n",
    "    \n",
    "    return timedelta(minutes=total_minutes)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_parquet('/workspaces/codespaces-jupyter/data/train-00000-of-00001-5e7cb295b9cff0bf.parquet').head(40)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(df)\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Process GPT and TextGrad responses with threading\n",
    "    results = []\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            future_to_index = {executor.submit(process_sample, row, start_time): idx \n",
    "                             for idx, row in df.iterrows()}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_index), total=total_samples):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    # Initialize metrics dict for each result\n",
    "                    result['metrics'] = {\n",
    "                        'gpt': calculate_metrics(result['reference_output'], result['responses']['gpt']),\n",
    "                        'textgrad': calculate_metrics(result['reference_output'], result['responses']['textgrad']),\n",
    "                        'dspy': None\n",
    "                    }\n",
    "                    results.append(result)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in threaded processing: {str(e)}\")\n",
    "\n",
    "    # Now process DSPy responses sequentially\n",
    "    if results:\n",
    "        try:\n",
    "            # Configure DSPy with the new LM client\n",
    "            with key_lock:\n",
    "                api_key = next(api_key_cycle)\n",
    "            \n",
    "            # Use the new DSPy LM client\n",
    "            lm = dspy.LM(\n",
    "                model=\"llama-3.1-70b-versatile\",\n",
    "                api_key=api_key,\n",
    "                api_base=\"https://api.groq.com/openai/v1/\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "                top_p=0.95,\n",
    "                presence_penalty=0.1,\n",
    "                frequency_penalty=0.1\n",
    "            )\n",
    "            dspy.settings.configure(lm=lm)\n",
    "            \n",
    "            # Define the DSPy program\n",
    "            class MedicalResponse(dspy.Signature):\n",
    "                \"\"\"Generate comprehensive medical advice for given symptoms.\"\"\"\n",
    "                input = dspy.InputField(desc=\"Patient's symptoms and concerns\")\n",
    "                response = dspy.OutputField(desc=\"\"\"\n",
    "                Provide a concise yet comprehensive medical response that includes:\n",
    "                1. Direct assessment of the condition\n",
    "                2. Expected duration and course\n",
    "                3. Clear guidance on when to seek immediate medical attention\n",
    "                4. Specific treatment recommendations\n",
    "                5. Home care instructions\n",
    "                6. Preventive measures\n",
    "                \n",
    "                Response should:\n",
    "                - Start with a greeting and acknowledgment\n",
    "                - Use clear, authoritative medical language\n",
    "                - Provide specific durations (e.g., \"5-7 days\")\n",
    "                - Include medication recommendations when appropriate\n",
    "                - Mention specific symptoms that warrant urgent care\n",
    "                - End with a clear conclusion\n",
    "                Must maintain professional medical tone and be at least 1500 characters.\"\"\")\n",
    "\n",
    "            class MedicalAdvisor(dspy.Module):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "                    self.chain = dspy.ChainOfThought(MedicalResponse)\n",
    "                \n",
    "                def forward(self, input_text):\n",
    "                    enhanced_prompt = f\"\"\"As an experienced pediatrician, provide a professional medical response.\n",
    "\n",
    "                    Patient Description: {input_text}\n",
    "\n",
    "                    Format your response as follows:\n",
    "\n",
    "                    1. Greeting\n",
    "                    \"Thank you for consulting Chat Doctor.\"\n",
    "\n",
    "                    2. Initial Assessment\n",
    "                    - State the likely condition\n",
    "                    - Mention expected duration\n",
    "                    - Specify if immediate medical attention is needed\n",
    "\n",
    "                    3. Key Points\n",
    "                    - Mention specific symptoms that would require immediate medical attention\n",
    "                    - List what parents should monitor\n",
    "                    - Provide clear guidance on medication use/avoiding unnecessary medications\n",
    "\n",
    "                    4. Treatment Recommendations\n",
    "                    - Specific medications or supplements if needed\n",
    "                    - Dosage guidelines if applicable\n",
    "                    - Home care instructions\n",
    "\n",
    "                    5. Follow-up Care\n",
    "                    - When to seek medical attention\n",
    "                    - What symptoms would warrant immediate care\n",
    "                    - Prevention measures\n",
    "\n",
    "                    Remember to:\n",
    "                    - Be direct and authoritative\n",
    "                    - Use medical terminology with explanations\n",
    "                    - Provide specific timeframes\n",
    "                    - Include clear yes/no guidance on antibiotics\n",
    "                    - Mention specific supplements if appropriate\n",
    "                    - End with a clear conclusion\n",
    "\n",
    "                    Keep response professional and detailed while maintaining an accessible tone.\"\"\"\n",
    "        \n",
    "                    \n",
    "                    prediction = self.chain(input=enhanced_prompt)\n",
    "                    response = prediction.response\n",
    "                    \n",
    "                    if len(response) < 1500:\n",
    "                        enhanced_prompt += \"\\n\\nPlease provide more detailed information, including:\\n\"\n",
    "                        enhanced_prompt += \"- Specific symptom monitoring guidelines\\n\"\n",
    "                        enhanced_prompt += \"- Detailed home care instructions\\n\"\n",
    "                        enhanced_prompt += \"- Common complications to watch for\\n\"\n",
    "                        enhanced_prompt += \"- Long-term management strategies\\n\"\n",
    "                        prediction = self.chain(input=enhanced_prompt)\n",
    "                        response = prediction.response\n",
    "                    \n",
    "                    return response\n",
    "\n",
    "            # Process each sample sequentially\n",
    "            for result in results:\n",
    "                try:\n",
    "                    advisor = MedicalAdvisor()\n",
    "                    response = advisor(result['input'])\n",
    "                    if response and len(response) >= 1500:\n",
    "                        result['responses']['dspy'] = str(response)\n",
    "                        result['metrics']['dspy'] = calculate_metrics(\n",
    "                            result['reference_output'], \n",
    "                            result['responses']['dspy']\n",
    "                        )\n",
    "                        logging.info(f\"Successfully added DSPy response ({len(response)} chars) for sample {result['index']}\")\n",
    "                    else:\n",
    "                        logging.warning(f\"DSPy response too short ({len(response) if response else 0} chars) for sample {result['index']}\")\n",
    "                        result['responses']['dspy'] = None\n",
    "                        result['metrics']['dspy'] = calculate_metrics(\"\", None)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to get DSPy response for sample {result['index']}: {str(e)}\")\n",
    "                    result['responses']['dspy'] = None\n",
    "                    result['metrics']['dspy'] = calculate_metrics(\"\", None)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to configure DSPy: {str(e)}\")\n",
    "    \n",
    "    # Save results\n",
    "    try:\n",
    "        final_results = {\n",
    "            'metadata': {\n",
    "                'total_samples_processed': len(results),\n",
    "                'start_time': start_time.isoformat(),\n",
    "                'end_time': datetime.now().isoformat(),\n",
    "                'actual_duration': str(datetime.now() - start_time),\n",
    "            },\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        with open('final_results.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "        logging.info(\"Results saved to 'final_results.json'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save results: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
