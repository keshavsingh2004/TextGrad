{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.6.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (0.27.2)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: pydantic-core, distro, annotated-types, pydantic, groq\n",
      "\u001b[33m  WARNING: The script distro is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 distro-1.9.0 groq-0.12.0 pydantic-2.9.2 pydantic-core-2.23.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.54.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting textgrad\n",
      "  Downloading textgrad-0.1.5.tar.gz (65 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting yake\n",
      "  Downloading yake-0.4.8-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/codespace/.local/lib/python3.12/site-packages (from textgrad) (9.0.0)\n",
      "Collecting python-dotenv>=1.0.0 (from textgrad)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from textgrad) (2.2.2)\n",
      "Requirement already satisfied: platformdirs>=3.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from textgrad) (4.3.6)\n",
      "Collecting datasets>=2.14.6 (from textgrad)\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting diskcache>=5.6.3 (from textgrad)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting graphviz>=0.20.3 (from textgrad)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting gdown>=5.2.0 (from textgrad)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.12/site-packages (from textgrad) (10.4.0)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tabulate (from yake)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from yake) (2.1.1)\n",
      "Collecting segtok (from yake)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from yake) (3.2.1)\n",
      "Collecting jellyfish (from yake)\n",
      "  Downloading jellyfish-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/codespace/.local/lib/python3.12/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.14.6->textgrad)\n",
      "  Downloading pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.6->textgrad)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (2.32.3)\n",
      "Collecting xxhash (from datasets>=2.14.6->textgrad)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.14.6->textgrad)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.14.6->textgrad) (2024.2.0)\n",
      "Collecting aiohttp (from datasets>=2.14.6->textgrad)\n",
      "  Downloading aiohttp-3.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets>=2.14.6->textgrad)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from datasets>=2.14.6->textgrad) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (from gdown>=5.2.0->textgrad) (4.12.3)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.5.3->textgrad) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.14.6->textgrad)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.14.6->textgrad)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->textgrad) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.14.6->textgrad)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.14.6->textgrad)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.14.6->textgrad)\n",
      "  Downloading propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.14.6->textgrad)\n",
      "  Downloading yarl-1.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (66 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.6->textgrad) (2.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->gdown>=5.2.0->textgrad) (2.6)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown>=5.2.0->textgrad)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading openai-1.54.4-py3-none-any.whl (389 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Downloading jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading jellyfish-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (335 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading yarl-1.17.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Building wheels for collected packages: textgrad, rouge_score\n",
      "  Building wheel for textgrad (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for textgrad: filename=textgrad-0.1.5-py3-none-any.whl size=69629 sha256=593814373b83d4194c59fa2c0ed5f223a2251a55de939de5b74b9ed316e6ae60\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/bb/e0/6e/407259036bba91132d3ef74681c1b374e55aafc1571943f0fc\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=cfe384ae96e935ea64c2da0e87d26d79ea96d5ca0a28fa4e97d0b50dc993b5c4\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built textgrad rouge_score\n",
      "Installing collected packages: xxhash, typing-extensions, tabulate, regex, python-dotenv, PySocks, pyarrow, propcache, multidict, jiter, jellyfish, graphviz, frozenlist, diskcache, dill, click, aiohappyeyeballs, absl-py, yarl, segtok, nltk, multiprocess, huggingface-hub, aiosignal, yake, rouge_score, gdown, aiohttp, openai, datasets, textgrad\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed PySocks-1.7.1 absl-py-2.1.0 aiohappyeyeballs-2.4.3 aiohttp-3.11.2 aiosignal-1.3.1 click-8.1.7 datasets-3.1.0 dill-0.3.8 diskcache-5.6.3 frozenlist-1.5.0 gdown-5.2.0 graphviz-0.20.3 huggingface-hub-0.26.2 jellyfish-1.1.0 jiter-0.7.1 multidict-6.1.0 multiprocess-0.70.16 nltk-3.9.1 openai-1.54.4 propcache-0.2.0 pyarrow-18.0.0 python-dotenv-1.0.1 regex-2024.11.6 rouge_score-0.1.2 segtok-1.5.11 tabulate-0.9.0 textgrad-0.1.5 typing-extensions-4.12.2 xxhash-3.5.0 yake-0.4.8 yarl-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai textgrad nltk yake rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dspy\n",
      "  Downloading dspy-2.5.31-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting backoff (from dspy)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dspy) (3.1.0)\n",
      "Requirement already satisfied: diskcache in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dspy) (5.6.3)\n",
      "Requirement already satisfied: httpx in /home/codespace/.local/lib/python3.12/site-packages (from dspy) (0.27.2)\n",
      "Requirement already satisfied: joblib~=1.3 in /home/codespace/.local/lib/python3.12/site-packages (from dspy) (1.4.2)\n",
      "Collecting json-repair (from dspy)\n",
      "  Downloading json_repair-0.30.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting litellm==1.51.0 (from dspy)\n",
      "  Downloading litellm-1.51.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting magicattr~=0.1.6 (from dspy)\n",
      "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: openai in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dspy) (1.54.4)\n",
      "Collecting optuna (from dspy)\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dspy) (2.2.2)\n",
      "Requirement already satisfied: pydantic~=2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dspy) (2.9.2)\n",
      "Requirement already satisfied: regex in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dspy) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from dspy) (2.32.3)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/codespace/.local/lib/python3.12/site-packages (from dspy) (9.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dspy) (4.66.4)\n",
      "Collecting ujson (from dspy)\n",
      "  Downloading ujson-5.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from dspy) (4.6.0)\n",
      "Collecting asyncer==0.0.8 (from dspy)\n",
      "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/python/3.12.1/lib/python3.12/site-packages (from litellm==1.51.0->dspy) (3.11.2)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from litellm==1.51.0->dspy) (8.1.7)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm==1.51.0->dspy)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/codespace/.local/lib/python3.12/site-packages (from litellm==1.51.0->dspy) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from litellm==1.51.0->dspy) (4.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from litellm==1.51.0->dspy) (1.0.1)\n",
      "Collecting tiktoken>=0.7.0 (from litellm==1.51.0->dspy)\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tokenizers (from litellm==1.51.0->dspy)\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->dspy) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->dspy) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai->dspy) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai->dspy) (0.7.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai->dspy) (4.12.2)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx->dspy) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx->dspy) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx->dspy) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic~=2.0->dspy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic~=2.0->dspy) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->dspy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->dspy) (2.2.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from datasets->dspy) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from datasets->dspy) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets->dspy) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets->dspy) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets->dspy) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets->dspy) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->dspy) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from datasets->dspy) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from datasets->dspy) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from datasets->dspy) (6.0.2)\n",
      "Collecting alembic>=1.5.0 (from optuna->dspy)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna->dspy)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna->dspy)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->dspy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->dspy) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->dspy) (2024.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna->dspy)\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->litellm==1.51.0->dspy) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->litellm==1.51.0->dspy) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->litellm==1.51.0->dspy) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->litellm==1.51.0->dspy) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->litellm==1.51.0->dspy) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->litellm==1.51.0->dspy) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from aiohttp->litellm==1.51.0->dspy) (1.17.2)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm==1.51.0->dspy)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.51.0->dspy) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.51.0->dspy) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.51.0->dspy) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.51.0->dspy) (0.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->dspy) (1.16.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.4.2->optuna->dspy)\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Downloading dspy-2.5.31-py3-none-any.whl (333 kB)\n",
      "Downloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
      "Downloading litellm-1.51.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading json_repair-0.30.2-py3-none-any.whl (18 kB)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "Downloading ujson-5.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m613.1/613.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: magicattr, zipp, ujson, Mako, json-repair, greenlet, colorlog, backoff, tiktoken, sqlalchemy, importlib-metadata, asyncer, tokenizers, alembic, optuna, litellm, dspy\n",
      "Successfully installed Mako-1.3.6 alembic-1.14.0 asyncer-0.0.8 backoff-2.2.1 colorlog-6.9.0 dspy-2.5.31 greenlet-3.1.1 importlib-metadata-8.5.0 json-repair-0.30.2 litellm-1.51.0 magicattr-0.1.6 optuna-4.1.0 sqlalchemy-2.0.36 tiktoken-0.8.0 tokenizers-0.20.3 ujson-5.10.0 zipp-3.21.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/codespace/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]2024-11-17 20:37:44,204 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:44,351 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:44,544 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:44,703 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:46,092 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:46,094 - INFO - LLMCall function forward\n",
      "2024-11-17 20:37:46,094 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:37:46,106 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:46,107 - INFO - LLMCall function forward\n",
      "2024-11-17 20:37:46,108 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:37:46,857 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:46,859 - INFO - LLMCall function forward\n",
      "2024-11-17 20:37:46,859 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:37:47,021 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:47,023 - INFO - LLMCall function forward\n",
      "2024-11-17 20:37:47,024 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:37:48,448 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:48,450 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:37:48,451 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:37:48,505 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:48,507 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:37:48,508 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:37:49,195 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:49,197 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:37:49,198 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:37:49,761 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:49,763 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:37:49,764 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:37:49,764 - INFO - Successfully processed GPT and TextGrad for sample 3\n",
      "2024-11-17 20:37:49,772 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:37:49,820 - INFO - Using default tokenizer.\n",
      "  2%|▎         | 1/40 [00:08<05:50,  8.99s/it]2024-11-17 20:37:49,868 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:49,870 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:37:49,871 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:37:50,158 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:37:50,159 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 20:37:50,456 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:50,458 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:37:50,459 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:37:50,459 - INFO - Successfully processed GPT and TextGrad for sample 1\n",
      "2024-11-17 20:37:50,468 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:37:50,550 - INFO - Using default tokenizer.\n",
      "  5%|▌         | 2/40 [00:09<02:37,  4.14s/it]2024-11-17 20:37:50,770 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:50,772 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:37:50,772 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:37:50,773 - INFO - Successfully processed GPT and TextGrad for sample 2\n",
      "2024-11-17 20:37:50,782 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:37:50,859 - INFO - Using default tokenizer.\n",
      "  8%|▊         | 3/40 [00:10<01:28,  2.40s/it]2024-11-17 20:37:53,033 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:53,289 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:37:53,290 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 20:37:54,487 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:55,047 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:56,564 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:56,566 - INFO - LLMCall function forward\n",
      "2024-11-17 20:37:56,567 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:37:57,349 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:57,351 - INFO - LLMCall function forward\n",
      "2024-11-17 20:37:57,352 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:37:59,017 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:59,019 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:37:59,019 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:37:59,295 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:59,297 - INFO - LLMCall function forward\n",
      "2024-11-17 20:37:59,298 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:37:59,834 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:37:59,836 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:37:59,837 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:00,126 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:00,127 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:38:00,777 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:00,779 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:00,780 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:00,781 - INFO - Successfully processed GPT and TextGrad for sample 5\n",
      "2024-11-17 20:38:00,787 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:00,860 - INFO - Using default tokenizer.\n",
      " 10%|█         | 4/40 [00:20<03:14,  5.40s/it]2024-11-17 20:38:01,675 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:01,678 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:38:01,678 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:01,952 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:01,953 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 20:38:03,945 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:06,550 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:06,552 - INFO - LLMCall function forward\n",
      "2024-11-17 20:38:06,552 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:38:07,385 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:07,386 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 20:38:08,470 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:08,470 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:38:09,008 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:09,010 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:38:09,011 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:10,633 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:10,635 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:10,636 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:10,636 - INFO - Successfully processed GPT and TextGrad for sample 7\n",
      "2024-11-17 20:38:10,644 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:10,740 - INFO - Using default tokenizer.\n",
      " 12%|█▎        | 5/40 [00:29<04:05,  7.02s/it]2024-11-17 20:38:14,156 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:15,949 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:15,951 - INFO - LLMCall function forward\n",
      "2024-11-17 20:38:15,954 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:38:16,212 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:16,213 - INFO - Retrying request to /chat/completions in 6.000000 seconds\n",
      "2024-11-17 20:38:17,775 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:18,262 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:18,263 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:38:19,040 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:19,041 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:38:23,917 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:23,919 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:23,919 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:23,920 - INFO - Successfully processed GPT and TextGrad for sample 6\n",
      "2024-11-17 20:38:23,927 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:24,040 - INFO - Using default tokenizer.\n",
      " 15%|█▌        | 6/40 [00:43<05:11,  9.15s/it]2024-11-17 20:38:24,348 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:24,350 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:38:24,351 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:24,618 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:24,619 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 20:38:27,326 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:27,326 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:38:27,367 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:27,546 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:28,827 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:28,828 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 20:38:29,299 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:29,301 - INFO - LLMCall function forward\n",
      "2024-11-17 20:38:29,302 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:38:31,695 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:31,697 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:38:31,697 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:32,350 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:32,352 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:32,353 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:32,353 - INFO - Successfully processed GPT and TextGrad for sample 9\n",
      "2024-11-17 20:38:32,360 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:32,444 - INFO - Using default tokenizer.\n",
      " 18%|█▊        | 7/40 [00:51<04:53,  8.90s/it]2024-11-17 20:38:36,648 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:38,446 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:38,448 - INFO - LLMCall function forward\n",
      "2024-11-17 20:38:38,449 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:38:38,705 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:38,706 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 20:38:42,244 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:42,246 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:42,247 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:42,248 - INFO - Successfully processed GPT and TextGrad for sample 8\n",
      "2024-11-17 20:38:42,257 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:42,345 - INFO - Using default tokenizer.\n",
      " 20%|██        | 8/40 [01:01<04:55,  9.22s/it]2024-11-17 20:38:42,468 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:42,470 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:42,471 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:42,471 - INFO - Successfully processed GPT and TextGrad for sample 4\n",
      "2024-11-17 20:38:42,477 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:42,566 - INFO - Using default tokenizer.\n",
      " 22%|██▎       | 9/40 [01:01<03:18,  6.41s/it]2024-11-17 20:38:43,102 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:43,104 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:43,104 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:43,105 - INFO - Successfully processed GPT and TextGrad for sample 0\n",
      "2024-11-17 20:38:43,111 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:43,202 - INFO - Using default tokenizer.\n",
      " 25%|██▌       | 10/40 [01:02<02:19,  4.64s/it]2024-11-17 20:38:44,742 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:44,743 - INFO - Retrying request to /chat/completions in 29.000000 seconds\n",
      "2024-11-17 20:38:45,804 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:45,980 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:45,982 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:38:45,982 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:46,270 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:46,271 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:38:46,630 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:47,463 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:47,465 - INFO - LLMCall function forward\n",
      "2024-11-17 20:38:47,466 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:38:48,755 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:48,757 - INFO - LLMCall function forward\n",
      "2024-11-17 20:38:48,758 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:38:50,552 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:50,554 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:38:50,555 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:51,210 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:51,213 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:38:51,214 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:38:51,498 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:51,499 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-11-17 20:38:52,429 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:52,431 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:38:52,432 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:38:52,433 - INFO - Successfully processed GPT and TextGrad for sample 11\n",
      "2024-11-17 20:38:52,439 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:38:52,517 - INFO - Using default tokenizer.\n",
      " 28%|██▊       | 11/40 [01:11<02:55,  6.06s/it]2024-11-17 20:38:56,263 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:58,095 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:38:58,097 - INFO - LLMCall function forward\n",
      "2024-11-17 20:38:58,098 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:38:58,359 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:38:58,360 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:39:00,540 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:00,541 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:39:10,588 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:10,590 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:39:10,591 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:39:10,893 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:10,894 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 20:39:11,082 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:11,084 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:39:11,084 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:39:11,085 - INFO - Successfully processed GPT and TextGrad for sample 10\n",
      "2024-11-17 20:39:11,092 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:39:11,168 - INFO - Using default tokenizer.\n",
      " 30%|███       | 12/40 [01:30<04:36,  9.89s/it]2024-11-17 20:39:12,957 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:12,959 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:39:12,959 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:39:12,960 - INFO - Successfully processed GPT and TextGrad for sample 13\n",
      "2024-11-17 20:39:12,967 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:39:13,056 - INFO - Using default tokenizer.\n",
      " 32%|███▎      | 13/40 [01:32<03:21,  7.47s/it]2024-11-17 20:39:14,956 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:14,984 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:15,241 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:15,242 - INFO - Retrying request to /chat/completions in 3.000000 seconds\n",
      "2024-11-17 20:39:15,248 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:15,249 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 20:39:16,588 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:16,590 - INFO - LLMCall function forward\n",
      "2024-11-17 20:39:16,591 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:39:16,871 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:16,872 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 20:39:19,680 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:19,944 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:19,946 - INFO - Retrying request to /chat/completions in 3.000000 seconds\n",
      "2024-11-17 20:39:20,828 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:20,830 - INFO - LLMCall function forward\n",
      "2024-11-17 20:39:20,831 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:39:21,109 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:21,110 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 20:39:23,192 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:23,193 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 20:39:27,174 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:27,175 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:39:29,144 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:29,145 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 20:39:30,138 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:30,140 - INFO - LLMCall function forward\n",
      "2024-11-17 20:39:30,141 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:39:30,412 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:30,412 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 20:39:39,389 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:39,390 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 20:39:39,412 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:40,668 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:40,669 - INFO - Retrying request to /chat/completions in 3.000000 seconds\n",
      "2024-11-17 20:39:42,685 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:42,687 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:39:42,687 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:39:42,688 - INFO - Successfully processed GPT and TextGrad for sample 14\n",
      "2024-11-17 20:39:42,694 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:39:42,779 - INFO - Using default tokenizer.\n",
      " 35%|███▌      | 14/40 [02:01<06:08, 14.19s/it]2024-11-17 20:39:44,642 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:44,644 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:39:44,645 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:39:44,903 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:44,904 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 20:39:45,457 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:45,459 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:39:45,460 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:39:45,509 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:45,511 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:39:45,512 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:39:45,757 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:45,758 - INFO - Retrying request to /chat/completions in 41.000000 seconds\n",
      "2024-11-17 20:39:46,683 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:46,810 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:46,812 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:39:46,812 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:39:46,813 - INFO - Successfully processed GPT and TextGrad for sample 15\n",
      "2024-11-17 20:39:46,820 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:39:46,906 - INFO - Using default tokenizer.\n",
      " 38%|███▊      | 15/40 [02:06<04:38, 11.15s/it]2024-11-17 20:39:49,111 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:49,113 - INFO - LLMCall function forward\n",
      "2024-11-17 20:39:49,113 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:39:50,657 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:51,965 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:52,002 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:39:52,002 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:39:53,582 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:53,598 - INFO - LLMCall function forward\n",
      "2024-11-17 20:39:53,598 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:39:53,862 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:39:53,862 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 20:39:54,260 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:54,262 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:39:54,262 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:39:54,263 - INFO - Successfully processed GPT and TextGrad for sample 17\n",
      "2024-11-17 20:39:54,271 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:39:54,383 - INFO - Using default tokenizer.\n",
      " 40%|████      | 16/40 [02:13<04:01, 10.06s/it]2024-11-17 20:39:57,643 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:59,675 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:39:59,678 - INFO - LLMCall function forward\n",
      "2024-11-17 20:39:59,679 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:40:00,189 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:00,189 - INFO - Retrying request to /chat/completions in 26.000000 seconds\n",
      "2024-11-17 20:40:02,124 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:02,126 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:40:02,126 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:40:03,811 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:03,813 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:40:03,813 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:40:03,814 - INFO - Successfully processed GPT and TextGrad for sample 19\n",
      "2024-11-17 20:40:03,820 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:40:03,897 - INFO - Using default tokenizer.\n",
      " 42%|████▎     | 17/40 [02:23<03:47,  9.88s/it]2024-11-17 20:40:06,114 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:06,115 - INFO - Retrying request to /chat/completions in 6.000000 seconds\n",
      "2024-11-17 20:40:07,111 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:07,113 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:40:07,114 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:40:07,405 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:07,406 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 20:40:13,912 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:14,167 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:14,167 - INFO - Retrying request to /chat/completions in 4.000000 seconds\n",
      "2024-11-17 20:40:19,972 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:19,974 - INFO - LLMCall function forward\n",
      "2024-11-17 20:40:19,975 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:40:20,251 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:20,252 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:40:24,668 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:24,670 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:40:24,671 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:40:24,671 - INFO - Successfully processed GPT and TextGrad for sample 18\n",
      "2024-11-17 20:40:24,678 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:40:24,759 - INFO - Using default tokenizer.\n",
      " 45%|████▌     | 18/40 [02:43<04:50, 13.18s/it]2024-11-17 20:40:26,464 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:27,049 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:27,050 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 20:40:27,740 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:27,741 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:40:28,101 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:31,601 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:31,603 - INFO - LLMCall function forward\n",
      "2024-11-17 20:40:31,604 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:40:34,410 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:34,412 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:40:34,412 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:40:35,248 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:35,250 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:40:35,251 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:40:35,251 - INFO - Successfully processed GPT and TextGrad for sample 21\n",
      "2024-11-17 20:40:35,257 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:40:35,303 - INFO - Using default tokenizer.\n",
      " 48%|████▊     | 19/40 [02:54<04:19, 12.38s/it]2024-11-17 20:40:35,529 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:35,531 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:40:35,532 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:40:35,805 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:35,806 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 20:40:37,030 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:37,031 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:40:37,322 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:38,586 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:38,587 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 20:40:38,625 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:39,899 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:39,901 - INFO - LLMCall function forward\n",
      "2024-11-17 20:40:39,901 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:40:41,778 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:41,780 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:40:41,781 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:40:42,034 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:42,035 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:40:51,590 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:51,592 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:40:51,592 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:40:51,593 - INFO - Successfully processed GPT and TextGrad for sample 16\n",
      "2024-11-17 20:40:51,599 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:40:51,665 - INFO - Using default tokenizer.\n",
      " 50%|█████     | 20/40 [03:10<04:31, 13.58s/it]2024-11-17 20:40:52,115 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:52,116 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:40:52,315 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:52,317 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:40:52,317 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:40:52,318 - INFO - Successfully processed GPT and TextGrad for sample 12\n",
      "2024-11-17 20:40:52,324 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:40:52,414 - INFO - Using default tokenizer.\n",
      " 52%|█████▎    | 21/40 [03:11<03:05,  9.74s/it]2024-11-17 20:40:54,597 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:40:54,598 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 20:40:55,263 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:56,376 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:56,378 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:40:56,378 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:40:56,379 - INFO - Successfully processed GPT and TextGrad for sample 22\n",
      "2024-11-17 20:40:56,386 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:40:56,457 - INFO - Using default tokenizer.\n",
      " 55%|█████▌    | 22/40 [03:15<02:24,  8.02s/it]2024-11-17 20:40:56,978 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:56,980 - INFO - LLMCall function forward\n",
      "2024-11-17 20:40:56,981 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:40:59,832 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:59,876 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:40:59,877 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:40:59,878 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:41:01,271 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:01,273 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:41:01,274 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:41:01,274 - INFO - Successfully processed GPT and TextGrad for sample 23\n",
      "2024-11-17 20:41:01,282 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:41:01,365 - INFO - Using default tokenizer.\n",
      " 57%|█████▊    | 23/40 [03:20<02:00,  7.09s/it]2024-11-17 20:41:02,274 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:02,276 - INFO - LLMCall function forward\n",
      "2024-11-17 20:41:02,277 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:41:04,759 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:04,955 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:04,957 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:41:04,958 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:41:05,220 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:05,221 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 20:41:06,413 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:06,561 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:06,563 - INFO - LLMCall function forward\n",
      "2024-11-17 20:41:06,564 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:41:06,842 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:06,843 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:41:07,700 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:07,701 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:41:07,908 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:08,806 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:08,807 - INFO - Retrying request to /chat/completions in 3.000000 seconds\n",
      "2024-11-17 20:41:14,003 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:14,005 - INFO - LLMCall function forward\n",
      "2024-11-17 20:41:14,005 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:41:14,269 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:14,270 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:41:16,645 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:16,647 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:41:16,648 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:41:16,914 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:16,915 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 20:41:17,059 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:17,061 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:41:17,061 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:41:17,062 - INFO - Successfully processed GPT and TextGrad for sample 25\n",
      "2024-11-17 20:41:17,071 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:41:17,139 - INFO - Using default tokenizer.\n",
      " 60%|██████    | 24/40 [03:36<02:35,  9.70s/it]2024-11-17 20:41:20,519 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:21,973 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:21,975 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 20:41:22,497 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:22,499 - INFO - LLMCall function forward\n",
      "2024-11-17 20:41:22,500 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:41:26,692 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:26,694 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:41:26,694 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:41:28,169 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:28,171 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:41:28,171 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:41:28,172 - INFO - Successfully processed GPT and TextGrad for sample 27\n",
      "2024-11-17 20:41:28,178 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:41:28,241 - INFO - Using default tokenizer.\n",
      " 62%|██████▎   | 25/40 [03:47<02:31, 10.11s/it]2024-11-17 20:41:30,532 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:30,534 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:41:30,535 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:41:30,799 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:30,800 - INFO - Retrying request to /chat/completions in 20.000000 seconds\n",
      "2024-11-17 20:41:31,671 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:31,925 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:31,926 - INFO - Retrying request to /chat/completions in 6.000000 seconds\n",
      "2024-11-17 20:41:32,267 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:33,209 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:33,211 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:41:33,211 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:41:33,212 - INFO - Successfully processed GPT and TextGrad for sample 26\n",
      "2024-11-17 20:41:33,218 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:41:33,277 - INFO - Using default tokenizer.\n",
      " 65%|██████▌   | 26/40 [03:52<02:00,  8.58s/it]2024-11-17 20:41:33,836 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:33,837 - INFO - Retrying request to /chat/completions in 18.000000 seconds\n",
      "2024-11-17 20:41:36,974 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:39,654 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:39,657 - INFO - LLMCall function forward\n",
      "2024-11-17 20:41:39,657 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:41:40,298 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:40,300 - INFO - LLMCall function forward\n",
      "2024-11-17 20:41:40,300 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:41:42,076 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:42,079 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:41:42,079 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:41:42,340 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:42,341 - INFO - Retrying request to /chat/completions in 17.000000 seconds\n",
      "2024-11-17 20:41:42,573 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:42,576 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:41:42,576 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:41:42,839 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:42,840 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:41:51,073 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:51,073 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:41:52,143 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:52,144 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 20:41:57,118 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:41:57,120 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:41:57,120 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:41:57,121 - INFO - Successfully processed GPT and TextGrad for sample 28\n",
      "2024-11-17 20:41:57,127 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:41:57,211 - INFO - Using default tokenizer.\n",
      " 68%|██████▊   | 27/40 [04:16<02:51, 13.21s/it]2024-11-17 20:41:57,523 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:59,615 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:41:59,616 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:42:00,356 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:00,778 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:00,779 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 20:42:01,442 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:01,622 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:01,623 - INFO - Retrying request to /chat/completions in 12.000000 seconds\n",
      "2024-11-17 20:42:03,840 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:03,842 - INFO - LLMCall function forward\n",
      "2024-11-17 20:42:03,843 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:42:07,008 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:07,010 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:42:07,010 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:42:07,272 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:07,273 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:42:13,178 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:13,180 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:42:13,180 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:42:13,181 - INFO - Successfully processed GPT and TextGrad for sample 20\n",
      "2024-11-17 20:42:13,188 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:42:13,321 - INFO - Using default tokenizer.\n",
      " 70%|███████   | 28/40 [04:32<02:48, 14.07s/it]2024-11-17 20:42:13,931 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:13,932 - INFO - Retrying request to /chat/completions in 15.000000 seconds\n",
      "2024-11-17 20:42:15,137 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:15,139 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:42:15,140 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:42:15,141 - INFO - Successfully processed GPT and TextGrad for sample 29\n",
      "2024-11-17 20:42:15,148 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:42:15,232 - INFO - Using default tokenizer.\n",
      " 72%|███████▎  | 29/40 [04:34<01:54, 10.42s/it]2024-11-17 20:42:16,290 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:16,292 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:42:16,292 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:42:16,293 - INFO - Successfully processed GPT and TextGrad for sample 30\n",
      "2024-11-17 20:42:16,300 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:42:16,391 - INFO - Using default tokenizer.\n",
      " 75%|███████▌  | 30/40 [04:35<01:16,  7.64s/it]2024-11-17 20:42:16,541 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:19,110 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:19,576 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:19,578 - INFO - LLMCall function forward\n",
      "2024-11-17 20:42:19,578 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:42:19,709 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:19,851 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:19,851 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:42:21,405 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:21,407 - INFO - LLMCall function forward\n",
      "2024-11-17 20:42:21,408 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:42:21,546 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:21,548 - INFO - LLMCall function forward\n",
      "2024-11-17 20:42:21,548 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:42:21,701 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:21,702 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:42:21,834 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:21,835 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:42:28,144 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:28,145 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 20:42:29,238 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:30,700 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:30,702 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:42:35,279 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:35,281 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:42:35,282 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:42:36,814 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:36,816 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:42:36,817 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:42:36,821 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:36,823 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:42:36,823 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:42:36,824 - INFO - Successfully processed GPT and TextGrad for sample 31\n",
      "2024-11-17 20:42:36,831 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:42:36,922 - INFO - Using default tokenizer.\n",
      " 78%|███████▊  | 31/40 [04:56<01:43, 11.52s/it]2024-11-17 20:42:37,103 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:37,104 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:42:38,474 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:38,476 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:42:38,476 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:42:38,768 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:38,769 - INFO - Retrying request to /chat/completions in 32.000000 seconds\n",
      "2024-11-17 20:42:39,976 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:39,977 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:42:40,583 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:43,021 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:43,023 - INFO - LLMCall function forward\n",
      "2024-11-17 20:42:43,023 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:42:46,661 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:46,663 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:42:46,664 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:42:46,951 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:46,952 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:42:50,876 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:50,877 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:42:50,878 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:42:50,878 - INFO - Successfully processed GPT and TextGrad for sample 32\n",
      "2024-11-17 20:42:50,885 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:42:50,974 - INFO - Using default tokenizer.\n",
      " 80%|████████  | 32/40 [05:10<01:38, 12.26s/it]2024-11-17 20:42:54,302 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:55,274 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:56,179 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:42:56,180 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 20:42:57,851 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:57,853 - INFO - LLMCall function forward\n",
      "2024-11-17 20:42:57,854 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:42:58,477 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:42:58,479 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:42:58,480 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:42:58,480 - INFO - Successfully processed GPT and TextGrad for sample 34\n",
      "2024-11-17 20:42:58,486 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:42:58,940 - INFO - Using default tokenizer.\n",
      " 82%|████████▎ | 33/40 [05:18<01:16, 10.99s/it]2024-11-17 20:43:00,090 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:00,092 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:43:00,092 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:43:01,249 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:01,251 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:43:01,251 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:43:01,252 - INFO - Successfully processed GPT and TextGrad for sample 35\n",
      "2024-11-17 20:43:01,259 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:43:01,347 - INFO - Using default tokenizer.\n",
      " 85%|████████▌ | 34/40 [05:20<00:50,  8.41s/it]2024-11-17 20:43:02,376 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:04,282 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:04,284 - INFO - LLMCall function forward\n",
      "2024-11-17 20:43:04,285 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:43:04,527 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:04,563 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:04,564 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:43:06,294 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:06,297 - INFO - LLMCall function forward\n",
      "2024-11-17 20:43:06,298 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:43:06,562 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:06,563 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 20:43:07,486 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:07,487 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 20:43:11,051 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:11,052 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:43:13,879 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:13,882 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:43:13,882 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:43:14,746 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:14,748 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:43:14,748 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:43:14,798 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:14,799 - INFO - Retrying request to /chat/completions in 13.000000 seconds\n",
      "2024-11-17 20:43:15,014 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:15,014 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 20:43:18,793 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:19,331 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:20,586 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:20,587 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 20:43:23,592 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:23,593 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:43:28,997 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:28,999 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:43:29,000 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:43:29,000 - INFO - Successfully processed GPT and TextGrad for sample 37\n",
      "2024-11-17 20:43:29,007 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:43:29,091 - INFO - Using default tokenizer.\n",
      " 88%|████████▊ | 35/40 [05:48<01:11, 14.21s/it]2024-11-17 20:43:31,863 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:31,864 - INFO - Retrying request to /chat/completions in 14.000000 seconds\n",
      "2024-11-17 20:43:32,265 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:32,267 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:43:32,267 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:43:32,268 - INFO - Successfully processed GPT and TextGrad for sample 36\n",
      "2024-11-17 20:43:32,275 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:43:32,358 - INFO - Using default tokenizer.\n",
      " 90%|█████████ | 36/40 [05:51<00:43, 10.92s/it]2024-11-17 20:43:32,779 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:32,884 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:32,886 - INFO - Retrying request to /chat/completions in 16.000000 seconds\n",
      "2024-11-17 20:43:34,851 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:34,853 - INFO - LLMCall function forward\n",
      "2024-11-17 20:43:34,854 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:43:35,856 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:37,299 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:37,301 - INFO - LLMCall function forward\n",
      "2024-11-17 20:43:37,302 - INFO - _backward_through_llm prompt\n",
      "2024-11-17 20:43:38,270 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:38,272 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:43:38,273 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:43:38,541 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:38,542 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:43:39,332 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:39,334 - INFO - _backward_through_llm gradient\n",
      "2024-11-17 20:43:39,335 - INFO - TextualGradientDescent prompt for update\n",
      "2024-11-17 20:43:40,736 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:40,738 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:43:40,739 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:43:40,740 - INFO - Successfully processed GPT and TextGrad for sample 39\n",
      "2024-11-17 20:43:40,746 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:43:40,834 - INFO - Using default tokenizer.\n",
      " 92%|█████████▎| 37/40 [06:00<00:30, 10.19s/it]2024-11-17 20:43:48,577 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:48,579 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:43:48,580 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:43:48,580 - INFO - Successfully processed GPT and TextGrad for sample 38\n",
      "2024-11-17 20:43:48,588 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:43:48,699 - INFO - Using default tokenizer.\n",
      " 95%|█████████▌| 38/40 [06:07<00:19,  9.50s/it]2024-11-17 20:43:48,949 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:48,951 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:43:48,951 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:43:48,952 - INFO - Successfully processed GPT and TextGrad for sample 33\n",
      "2024-11-17 20:43:48,958 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:43:49,044 - INFO - Using default tokenizer.\n",
      " 98%|█████████▊| 39/40 [06:08<00:06,  6.76s/it]2024-11-17 20:43:50,980 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 20:43:50,982 - INFO - TextualGradientDescent optimizer response\n",
      "2024-11-17 20:43:50,982 - INFO - TextualGradientDescent updated text\n",
      "2024-11-17 20:43:50,983 - INFO - Successfully processed GPT and TextGrad for sample 24\n",
      "2024-11-17 20:43:50,989 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:43:51,076 - INFO - Using default tokenizer.\n",
      "100%|██████████| 40/40 [06:10<00:00,  9.26s/it]\n",
      "\u001b[92m20:43:51 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:43:51,165 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:43:51,442 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:43:51,443 - INFO - Retrying request to /chat/completions in 5.000000 seconds\n",
      "2024-11-17 20:44:00,533 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:44:00 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:00,536 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:00,553 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:44:00,624 - INFO - Successfully added DSPy response (4340 chars) for sample 3\n",
      "\u001b[92m20:44:00 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:00,627 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:00,876 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:44:00,877 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:44:12,790 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:44:12 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:12,793 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:12,803 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:44:12,887 - INFO - Successfully added DSPy response (2683 chars) for sample 1\n",
      "\u001b[92m20:44:12 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:12,889 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:13,145 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:44:13,146 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:44:24,194 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:44:24 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:24,197 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:24,207 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:44:24,316 - INFO - Successfully added DSPy response (2879 chars) for sample 2\n",
      "\u001b[92m20:44:24 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:24,319 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:24,572 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:44:24,573 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:44:35,424 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:44:35 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:35,427 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:35,437 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:44:35,519 - INFO - Successfully added DSPy response (2814 chars) for sample 5\n",
      "\u001b[92m20:44:35 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:35,522 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:35,784 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:44:35,785 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:44:47,070 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:44:47 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:47,072 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:47,082 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:44:47,168 - INFO - Successfully added DSPy response (2166 chars) for sample 7\n",
      "\u001b[92m20:44:47 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:47,170 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:47,419 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:44:47,419 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:44:57,798 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:44:57 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:57,800 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:44:57,811 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:44:57,912 - INFO - Successfully added DSPy response (2395 chars) for sample 6\n",
      "\u001b[92m20:44:57 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:57,914 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:44:58,162 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:44:58,163 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:45:09,368 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:45:09 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:09,371 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:09,382 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:45:09,453 - INFO - Successfully added DSPy response (3026 chars) for sample 9\n",
      "\u001b[92m20:45:09 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:09,456 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:09,714 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:45:09,715 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:45:19,943 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:45:19 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:19,946 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:19,957 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:45:20,056 - INFO - Successfully added DSPy response (3073 chars) for sample 8\n",
      "\u001b[92m20:45:20 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:20,059 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:20,316 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:45:20,317 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:45:31,329 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:45:31 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:31,332 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:31,342 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:45:31,441 - INFO - Successfully added DSPy response (2680 chars) for sample 4\n",
      "\u001b[92m20:45:31 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:31,443 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:31,694 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:45:31,695 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:45:42,310 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:45:42 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:42,313 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:42,323 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:45:42,422 - INFO - Successfully added DSPy response (2171 chars) for sample 0\n",
      "\u001b[92m20:45:42 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:42,425 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:42,687 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:45:42,688 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:45:54,475 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:45:54 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:54,477 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:45:54,489 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:45:54,575 - INFO - Successfully added DSPy response (2793 chars) for sample 11\n",
      "\u001b[92m20:45:54 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:54,577 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:45:55,474 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:45:55,475 - INFO - Retrying request to /chat/completions in 6.000000 seconds\n",
      "2024-11-17 20:46:04,100 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:46:04 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:04,102 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:04,111 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:46:04,206 - INFO - Successfully added DSPy response (2385 chars) for sample 10\n",
      "\u001b[92m20:46:04 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:04,208 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:04,457 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:46:04,458 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:46:15,674 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:46:15 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:15,676 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:15,689 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:46:15,791 - INFO - Successfully added DSPy response (2428 chars) for sample 13\n",
      "\u001b[92m20:46:15 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:15,793 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:16,048 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:46:16,049 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:46:26,134 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:46:26 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:26,136 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:26,147 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:46:26,246 - INFO - Successfully added DSPy response (2745 chars) for sample 14\n",
      "\u001b[92m20:46:26 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:26,248 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:26,502 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:46:26,503 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:46:37,662 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:46:37 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:37,665 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:37,677 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:46:37,785 - INFO - Successfully added DSPy response (3088 chars) for sample 15\n",
      "\u001b[92m20:46:37 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:37,787 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:38,037 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:46:38,037 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:46:49,473 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:46:49 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:49,475 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:46:49,486 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:46:49,616 - INFO - Successfully added DSPy response (2628 chars) for sample 17\n",
      "\u001b[92m20:46:49 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:49,619 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:46:49,882 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:46:49,882 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:47:01,146 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:47:01 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:01,149 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:01,163 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:47:01,292 - INFO - Successfully added DSPy response (3164 chars) for sample 19\n",
      "\u001b[92m20:47:01 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:01,294 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:02,189 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:47:02,190 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:47:12,276 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:47:12 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:12,278 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:12,288 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:47:12,375 - INFO - Successfully added DSPy response (2723 chars) for sample 18\n",
      "\u001b[92m20:47:12 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:12,377 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:12,627 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:47:12,628 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:47:23,733 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:47:23 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:23,735 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:23,745 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:47:23,816 - INFO - Successfully added DSPy response (2785 chars) for sample 21\n",
      "\u001b[92m20:47:23 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:23,818 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:24,065 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:47:24,066 - INFO - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2024-11-17 20:47:37,298 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:47:37 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:37,300 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:37,311 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:47:37,391 - INFO - Successfully added DSPy response (3163 chars) for sample 16\n",
      "\u001b[92m20:47:37 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:37,394 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:37,658 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:47:37,658 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:47:48,497 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:47:48 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:48,500 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:48,511 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:47:48,598 - INFO - Successfully added DSPy response (2779 chars) for sample 12\n",
      "\u001b[92m20:47:48 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:48,600 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:47:48,850 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:47:48,851 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:47:59,927 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:47:59 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:59,929 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:47:59,940 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:48:00,025 - INFO - Successfully added DSPy response (3063 chars) for sample 22\n",
      "\u001b[92m20:48:00 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:00,028 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:00,289 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:48:00,290 - INFO - Retrying request to /chat/completions in 9.000000 seconds\n",
      "2024-11-17 20:48:12,271 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:48:12 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:12,274 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:12,283 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:48:12,370 - INFO - Successfully added DSPy response (2122 chars) for sample 23\n",
      "\u001b[92m20:48:12 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:12,373 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:12,625 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:48:12,626 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:48:23,042 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:48:23 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:23,044 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:23,055 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:48:23,132 - INFO - Successfully added DSPy response (2529 chars) for sample 25\n",
      "\u001b[92m20:48:23 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:23,134 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:23,398 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:48:23,399 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:48:34,294 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:48:34 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:34,296 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:34,306 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:48:34,396 - INFO - Successfully added DSPy response (2876 chars) for sample 27\n",
      "\u001b[92m20:48:34 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:34,398 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:34,647 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:48:34,648 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:48:44,461 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:48:44 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:44,463 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:44,473 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:48:44,550 - INFO - Successfully added DSPy response (2358 chars) for sample 26\n",
      "\u001b[92m20:48:44 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:44,552 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:44,819 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:48:44,819 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:48:54,752 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:48:54 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:54,755 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:48:54,764 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:48:54,860 - INFO - Successfully added DSPy response (2069 chars) for sample 28\n",
      "\u001b[92m20:48:54 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:54,862 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:48:55,126 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:48:55,126 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:49:06,470 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:49:06 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:06,473 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:06,484 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:49:06,575 - INFO - Successfully added DSPy response (2935 chars) for sample 20\n",
      "\u001b[92m20:49:06 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:06,577 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:06,837 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:49:06,837 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:49:16,523 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:49:16 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:16,525 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:16,536 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:49:16,623 - INFO - Successfully added DSPy response (2331 chars) for sample 29\n",
      "\u001b[92m20:49:16 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:16,625 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:16,888 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:49:16,889 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:49:28,449 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:49:28 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:28,451 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:28,462 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:49:28,565 - INFO - Successfully added DSPy response (2578 chars) for sample 30\n",
      "\u001b[92m20:49:28 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:28,568 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:28,834 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:49:28,835 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:49:38,769 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:49:38 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:38,772 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:38,782 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:49:38,877 - INFO - Successfully added DSPy response (2708 chars) for sample 31\n",
      "\u001b[92m20:49:38 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:38,880 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:39,126 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:49:39,127 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:49:50,963 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:49:50 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:50,966 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:49:50,978 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:49:51,064 - INFO - Successfully added DSPy response (2763 chars) for sample 32\n",
      "\u001b[92m20:49:51 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:51,067 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:49:51,331 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:49:51,332 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:50:02,227 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:50:02 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:02,230 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:02,240 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:50:02,334 - INFO - Successfully added DSPy response (2527 chars) for sample 34\n",
      "\u001b[92m20:50:02 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:02,337 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:02,586 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:50:02,587 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:50:13,324 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:50:13 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:13,326 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:13,337 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:50:13,434 - INFO - Successfully added DSPy response (2598 chars) for sample 35\n",
      "\u001b[92m20:50:13 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:13,436 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:13,698 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:50:13,698 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:50:24,464 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:50:24 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:24,467 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:24,477 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:50:24,567 - INFO - Successfully added DSPy response (2451 chars) for sample 37\n",
      "\u001b[92m20:50:24 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:24,570 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:24,838 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:50:24,839 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:50:34,872 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:50:34 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:34,875 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:34,889 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:50:34,982 - INFO - Successfully added DSPy response (2776 chars) for sample 36\n",
      "\u001b[92m20:50:34 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:34,985 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:35,245 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:50:35,246 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:50:46,643 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:50:46 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:46,646 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:50:46,658 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:50:46,762 - INFO - Successfully added DSPy response (3420 chars) for sample 39\n",
      "\u001b[92m20:50:46 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:46,764 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:50:47,014 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:50:47,015 - INFO - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2024-11-17 20:51:01,868 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:51:01 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:51:01,870 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:51:01,882 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:51:02,003 - INFO - Successfully added DSPy response (2879 chars) for sample 38\n",
      "\u001b[92m20:51:02 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:51:02,006 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:51:02,258 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:51:02,259 - INFO - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2024-11-17 20:51:11,878 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:51:11 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:51:11,928 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:51:11,940 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:51:12,050 - INFO - Successfully added DSPy response (2487 chars) for sample 33\n",
      "\u001b[92m20:51:12 - LiteLLM:INFO\u001b[0m: utils.py:2749 - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:51:12,053 - INFO - \n",
      "LiteLLM completion() model= llama-3.2-90b-vision-preview; provider = groq\n",
      "2024-11-17 20:51:12,303 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2024-11-17 20:51:12,303 - INFO - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2024-11-17 20:51:23,533 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m20:51:23 - LiteLLM:INFO\u001b[0m: utils.py:944 - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:51:23,542 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2024-11-17 20:51:23,554 - INFO - Using default tokenizer.\n",
      "2024-11-17 20:51:23,668 - INFO - Successfully added DSPy response (3367 chars) for sample 24\n",
      "2024-11-17 20:51:23,676 - INFO - Results saved to 'vision_results.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import textgrad as tg\n",
    "from textgrad.engine.local_model_openai_api import ChatExternalClient\n",
    "from groq import Groq\n",
    "import dspy\n",
    "from dspy.primitives.program import Module\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import yake\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import itertools\n",
    "from pydantic import BaseModel, Field\n",
    "import threading\n",
    "from dspy.predict import ChainOfThought\n",
    "\n",
    "dspy_config_lock = threading.Lock()\n",
    "dspy_configured = threading.Event()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "key_lock = threading.Lock()\n",
    "\n",
    "# Rate limiting constants\n",
    "RATE_LIMIT = 30  # requests per minute per API key\n",
    "DELAY_BETWEEN_REQUESTS = 60 / RATE_LIMIT  # seconds between requests\n",
    "BATCH_SIZE = 25  # Slightly less than rate limit to account for overhead\n",
    "API_KEYS = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"\n",
    "]\n",
    "api_key_cycle = itertools.cycle(API_KEYS)\n",
    "key_lock = Lock()\n",
    "\n",
    "def ensure_nltk_data():\n",
    "    \"\"\"Ensure all required NLTK data is downloaded\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "\n",
    "def initialize_dspy(api_key):\n",
    "    \"\"\"Initialize DSPy configuration once\"\"\"\n",
    "    global dspy_configured\n",
    "    with dspy_config_lock:\n",
    "        if not dspy_configured.is_set():\n",
    "            try:\n",
    "                dspy.settings.configure(lm=dspy.OpenAI(\n",
    "                    model=\"llama-3.2-90b-vision-preview\",\n",
    "                    api_key=api_key,\n",
    "                    api_base=\"https://api.groq.com/openai/v1\"\n",
    "                ))\n",
    "                dspy_configured.set()\n",
    "                logging.info(\"DSPy configured successfully\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to configure DSPy: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "class Input(BaseModel):\n",
    "    context: str = Field(description=\"The context for the question\")\n",
    "    query: str = Field(description=\"The question to be answered\")\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str = Field(description=\"The answer for the question\")\n",
    "    confidence: float = Field(ge=0, le=1, description=\"The confidence score for the answer\")\n",
    "\n",
    "class MedicalResponse(dspy.Signature):\n",
    "    \"\"\"Generate accurate medical responses with reasoning.\"\"\"\n",
    "    input: Input = dspy.InputField()\n",
    "    output: Output = dspy.OutputField()\n",
    "\n",
    "class GroqPredictor(dspy.TypedPredictor):\n",
    "    \"\"\"Custom predictor for Groq API\"\"\"\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Groq(api_key=api_key)\n",
    "        \n",
    "    def forward(self, prompt):\n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=\"llama-3.2-90b-vision-preview\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Groq API call failed: {e}\")\n",
    "            return None\n",
    "\n",
    "def get_gpt_response(client, prompt, retry_count=3):\n",
    "    \"\"\"Get GPT response with rate limiting and retries\"\"\"\n",
    "    system_prompt = \"\"\"You are a medical expert assistant. Provide accurate, clear, and well-structured medical advice.\n",
    "Focus on: accuracy, clear explanation, practical advice, and professional yet accessible language.\n",
    "Please limit your response to a maximum of 1500 characters.\"\"\"\n",
    "    \n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama-3.2-90b-vision-preview\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"GPT attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(5)\n",
    "    return None\n",
    "\n",
    "def get_textgrad_response(client, prompt, reference, retry_count=3):\n",
    "    \"\"\"Get TextGrad optimized response\"\"\"\n",
    "    try:\n",
    "        engine = ChatExternalClient(client=client, model_string='llama-3.2-90b-vision-preview')\n",
    "        tg.set_backward_engine(engine, override=True)\n",
    "\n",
    "        loss_system_prompt = tg.Variable(\n",
    "            \"\"\"Evaluate the medical response based on the following criteria:\n",
    "            - Accuracy: Ensure all information is factually correct and evidence-based.\n",
    "            - Clarity: The response should be clear, concise, and easy to understand.\n",
    "            - Completeness: Address all aspects of the medical query comprehensively.\n",
    "            - Practicality: Provide actionable and practical advice that can be implemented.\n",
    "            - Professionalism: Maintain a professional tone and uphold medical ethical standards.\n",
    "            - Relevance: Ensure all information provided is directly related to the query.\n",
    "            - Consistency: Maintain consistency in terminology and presentation throughout the response.\n",
    "            \"\"\",\n",
    "            requires_grad=False,\n",
    "            role_description=\"medical evaluation system\"\n",
    "        )\n",
    "\n",
    "        solution = tg.Variable(reference, requires_grad=True, role_description=\"medical response\")\n",
    "        loss_fn = tg.TextLoss(loss_system_prompt)\n",
    "        optimizer = tg.TGD([solution])\n",
    "        loss = loss_fn(solution)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return solution.value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"TextGrad optimization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_dspy_response(api_key, input_text):\n",
    "    \"\"\"Get response using DSPy framework\"\"\"\n",
    "    try:\n",
    "        # Ensure DSPy is initialized\n",
    "        initialize_dspy(api_key)\n",
    "        \n",
    "        # Define the signature\n",
    "        class MedicalResponse(dspy.Signature):\n",
    "            input = dspy.InputField(desc=\"medical query or symptom description\")\n",
    "            response = dspy.OutputField(desc=\"detailed medical advice\")\n",
    "\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "        # Create program instance with proper error handling\n",
    "        try:\n",
    "            program = ChainOfThought(MedicalResponse)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to create ChainOfThought program: {str(e)}\")\n",
    "            return {'response': None}\n",
    "\n",
    "        # Generate response with timeout\n",
    "        try:\n",
    "            pred = program(input=str(input_text))\n",
    "            \n",
    "            if hasattr(pred, 'response') and pred.response:\n",
    "                response_text = str(pred.response).strip()\n",
    "                if response_text:\n",
    "                    logging.info(\"Successfully generated DSPy response\")\n",
    "                    return {'response': response_text}\n",
    "            \n",
    "            logging.warning(\"DSPy generated empty response\")\n",
    "            return {'response': None}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to generate DSPy response: {str(e)}\")\n",
    "            return {'response': None}\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"DSPy processing failed: {str(e)}\")\n",
    "        logging.error(f\"Input text was: {str(input_text)[:100]}...\")\n",
    "        return {'response': None}\n",
    "\n",
    "\n",
    "def calculate_metrics(reference, candidate):\n",
    "    \"\"\"Calculate various similarity metrics between reference and candidate texts\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if not reference or not candidate:\n",
    "        return {\n",
    "            'content_similarity': 0,\n",
    "            'word_overlap': 0,\n",
    "            'bleu_score': 0,\n",
    "            'rouge1': 0,\n",
    "            'rouge2': 0,\n",
    "            'rougeL': 0,\n",
    "            'keyword_overlap': 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Content Similarity\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([reference, candidate])\n",
    "        metrics['content_similarity'] = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        \n",
    "        # Word Overlap\n",
    "        ref_words = set(reference.lower().split())\n",
    "        cand_words = set(candidate.lower().split())\n",
    "        metrics['word_overlap'] = len(ref_words.intersection(cand_words)) / len(ref_words) if ref_words else 0\n",
    "        \n",
    "        # BLEU Score\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        metrics['bleu_score'] = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n",
    "        \n",
    "        # ROUGE Scores\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = scorer.score(reference, candidate)\n",
    "        metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
    "        metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
    "        metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
    "        \n",
    "        # Keyword Matching\n",
    "        kw_extractor = yake.KeywordExtractor()\n",
    "        ref_keywords = {kw[0] for kw in kw_extractor.extract_keywords(reference)}\n",
    "        cand_keywords = {kw[0] for kw in kw_extractor.extract_keywords(candidate)}\n",
    "        metrics['keyword_overlap'] = len(ref_keywords.intersection(cand_keywords)) / len(ref_keywords) if ref_keywords else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Metrics calculation failed: {e}\")\n",
    "        metrics = {k: 0 for k in ['content_similarity', 'word_overlap', 'bleu_score', 'rouge1', 'rouge2', 'rougeL', 'keyword_overlap']}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def limit_response(response, limit=1500):\n",
    "    \"\"\"Truncate the response to the specified character limit\"\"\"\n",
    "    return response[:limit] if response and len(response) > limit else response\n",
    "\n",
    "\n",
    "def process_sample(row, start_time):\n",
    "    \"\"\"Process a single sample without threading for DSPy\"\"\"\n",
    "    try:\n",
    "        with key_lock:\n",
    "            api_key = next(api_key_cycle)\n",
    "        \n",
    "        # Initialize clients\n",
    "        groq_client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=api_key)\n",
    "        \n",
    "        input_text = str(row['input']) if pd.notna(row['input']) else \"\"\n",
    "        output_text = str(row['output']) if pd.notna(row['output']) else \"\"\n",
    "        \n",
    "        # Get GPT and TextGrad responses (these are thread-safe)\n",
    "        gpt_response = None\n",
    "        textgrad_response = None\n",
    "        \n",
    "        try:\n",
    "            gpt_response = get_gpt_response(groq_client, input_text)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"GPT response failed: {str(e)}\")\n",
    "            \n",
    "        try:\n",
    "            textgrad_response = get_textgrad_response(groq_client, input_text, output_text)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"TextGrad response failed: {str(e)}\")\n",
    "        \n",
    "        result = {\n",
    "            'index': int(row.name) if isinstance(row.name, (int, float)) else 0,\n",
    "            'input': input_text,\n",
    "            'reference_output': output_text,\n",
    "            'responses': {\n",
    "                'gpt': str(gpt_response) if gpt_response is not None else None,\n",
    "                'textgrad': str(textgrad_response) if textgrad_response is not None else None,\n",
    "                'dspy': None  # We'll fill this in later\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Successfully processed GPT and TextGrad for sample {row.name}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing sample {row.name}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing sample {row.name}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def estimate_completion_time(total_samples):\n",
    "    \"\"\"Calculate estimated completion time based on rate limits\"\"\"\n",
    "    # Each sample needs 3 API calls (GPT, TextGrad, and DSPy)\n",
    "    total_requests = total_samples * 3\n",
    "    \n",
    "    # Calculate total minutes needed\n",
    "    total_minutes = total_requests / RATE_LIMIT\n",
    "    \n",
    "    # Add 20% buffer for overhead and potential retries\n",
    "    total_minutes *= 1.2\n",
    "    \n",
    "    return timedelta(minutes=total_minutes)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        df = pd.read_parquet('data/train-00000-of-00001-5e7cb295b9cff0bf.parquet').head(40)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(df)\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Process GPT and TextGrad responses with threading\n",
    "    results = []\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            future_to_index = {executor.submit(process_sample, row, start_time): idx \n",
    "                             for idx, row in df.iterrows()}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_index), total=total_samples):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    # Initialize metrics dict for each result\n",
    "                    result['metrics'] = {\n",
    "                        'gpt': calculate_metrics(result['reference_output'], result['responses']['gpt']),\n",
    "                        'textgrad': calculate_metrics(result['reference_output'], result['responses']['textgrad']),\n",
    "                        'dspy': None\n",
    "                    }\n",
    "                    results.append(result)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in threaded processing: {str(e)}\")\n",
    "\n",
    "    # Now process DSPy responses sequentially\n",
    "    if results:\n",
    "        try:\n",
    "            # Configure DSPy with the new LM client\n",
    "            with key_lock:\n",
    "                api_key = next(api_key_cycle)\n",
    "            \n",
    "            # Use the new DSPy LM client\n",
    "            lm = dspy.LM(\n",
    "                model=\"llama-3.2-90b-vision-preview\",\n",
    "                api_key=api_key,\n",
    "                api_base=\"https://api.groq.com/openai/v1/\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "                top_p=0.95,\n",
    "                presence_penalty=0.1,\n",
    "                frequency_penalty=0.1\n",
    "            )\n",
    "            dspy.settings.configure(lm=lm)\n",
    "            \n",
    "            # Define the DSPy program\n",
    "            class MedicalResponse(dspy.Signature):\n",
    "                \"\"\"Generate comprehensive medical advice for given symptoms.\"\"\"\n",
    "                input = dspy.InputField(desc=\"Patient's symptoms and concerns\")\n",
    "                response = dspy.OutputField(desc=\"\"\"\n",
    "                Provide a concise yet comprehensive medical response that includes:\n",
    "                1. Direct assessment of the condition\n",
    "                2. Expected duration and course\n",
    "                3. Clear guidance on when to seek immediate medical attention\n",
    "                4. Specific treatment recommendations\n",
    "                5. Home care instructions\n",
    "                6. Preventive measures\n",
    "                \n",
    "                Response should:\n",
    "                - Start with a greeting and acknowledgment\n",
    "                - Use clear, authoritative medical language\n",
    "                - Provide specific durations (e.g., \"5-7 days\")\n",
    "                - Include medication recommendations when appropriate\n",
    "                - Mention specific symptoms that warrant urgent care\n",
    "                - End with a clear conclusion\n",
    "                Must maintain professional medical tone and be at least 1500 characters.\"\"\")\n",
    "\n",
    "            class MedicalAdvisor(dspy.Module):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "                    self.chain = dspy.ChainOfThought(MedicalResponse)\n",
    "                \n",
    "                def forward(self, input_text):\n",
    "                    enhanced_prompt = f\"\"\"As an experienced pediatrician, provide a professional medical response.\n",
    "\n",
    "                    Patient Description: {input_text}\n",
    "\n",
    "                    Format your response as follows:\n",
    "\n",
    "                    1. Greeting\n",
    "                    \"Thank you for consulting Chat Doctor.\"\n",
    "\n",
    "                    2. Initial Assessment\n",
    "                    - State the likely condition\n",
    "                    - Mention expected duration\n",
    "                    - Specify if immediate medical attention is needed\n",
    "\n",
    "                    3. Key Points\n",
    "                    - Mention specific symptoms that would require immediate medical attention\n",
    "                    - List what parents should monitor\n",
    "                    - Provide clear guidance on medication use/avoiding unnecessary medications\n",
    "\n",
    "                    4. Treatment Recommendations\n",
    "                    - Specific medications or supplements if needed\n",
    "                    - Dosage guidelines if applicable\n",
    "                    - Home care instructions\n",
    "\n",
    "                    5. Follow-up Care\n",
    "                    - When to seek medical attention\n",
    "                    - What symptoms would warrant immediate care\n",
    "                    - Prevention measures\n",
    "\n",
    "                    Remember to:\n",
    "                    - Be direct and authoritative\n",
    "                    - Use medical terminology with explanations\n",
    "                    - Provide specific timeframes\n",
    "                    - Include clear yes/no guidance on antibiotics\n",
    "                    - Mention specific supplements if appropriate\n",
    "                    - End with a clear conclusion\n",
    "\n",
    "                    Keep response professional and detailed while maintaining an accessible tone.\"\"\"\n",
    "        \n",
    "                    \n",
    "                    prediction = self.chain(input=enhanced_prompt)\n",
    "                    response = prediction.response\n",
    "                    \n",
    "                    if len(response) < 1500:\n",
    "                        enhanced_prompt += \"\\n\\nPlease provide more detailed information, including:\\n\"\n",
    "                        enhanced_prompt += \"- Specific symptom monitoring guidelines\\n\"\n",
    "                        enhanced_prompt += \"- Detailed home care instructions\\n\"\n",
    "                        enhanced_prompt += \"- Common complications to watch for\\n\"\n",
    "                        enhanced_prompt += \"- Long-term management strategies\\n\"\n",
    "                        prediction = self.chain(input=enhanced_prompt)\n",
    "                        response = prediction.response\n",
    "                    \n",
    "                    return response\n",
    "\n",
    "            # Process each sample sequentially\n",
    "            for result in results:\n",
    "                try:\n",
    "                    advisor = MedicalAdvisor()\n",
    "                    response = advisor(result['input'])\n",
    "                    if response and len(response) >= 1500:\n",
    "                        result['responses']['dspy'] = str(response)\n",
    "                        result['metrics']['dspy'] = calculate_metrics(\n",
    "                            result['reference_output'], \n",
    "                            result['responses']['dspy']\n",
    "                        )\n",
    "                        logging.info(f\"Successfully added DSPy response ({len(response)} chars) for sample {result['index']}\")\n",
    "                    else:\n",
    "                        logging.warning(f\"DSPy response too short ({len(response) if response else 0} chars) for sample {result['index']}\")\n",
    "                        result['responses']['dspy'] = None\n",
    "                        result['metrics']['dspy'] = calculate_metrics(\"\", None)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to get DSPy response for sample {result['index']}: {str(e)}\")\n",
    "                    result['responses']['dspy'] = None\n",
    "                    result['metrics']['dspy'] = calculate_metrics(\"\", None)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to configure DSPy: {str(e)}\")\n",
    "    \n",
    "    # Save results\n",
    "    try:\n",
    "        final_results = {\n",
    "            'metadata': {\n",
    "                'total_samples_processed': len(results),\n",
    "                'start_time': start_time.isoformat(),\n",
    "                'end_time': datetime.now().isoformat(),\n",
    "                'actual_duration': str(datetime.now() - start_time),\n",
    "            },\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        with open('vision_results.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)\n",
    "        logging.info(\"Results saved to 'vision_results.json'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save results: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
